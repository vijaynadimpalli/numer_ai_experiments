{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "My WSB to  Numerai Signals - CLEANED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "792e07f9347f4e8db0e356a048c49b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c1e2c789c11433e82d1e265873cba24",
              "IPY_MODEL_1f600e114d594b5e8589f563c9e6c520"
            ],
            "layout": "IPY_MODEL_1a35ff6380c84e60822ab8111a003a2c"
          }
        },
        "8c1e2c789c11433e82d1e265873cba24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": " 87%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece858afd5814f31b1311b80ca54f2ac",
            "max": 244,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83ab13f9d5514c09b9a86c4ca799a844",
            "value": 212
          }
        },
        "1f600e114d594b5e8589f563c9e6c520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_986a3e51c9bd4ee5837f978f1ed07013",
            "placeholder": "​",
            "style": "IPY_MODEL_5f1c04f1529d4195acfa5a196f86ebc3",
            "value": " 212/244 [13:50&lt;02:12,  4.14s/it]"
          }
        },
        "1a35ff6380c84e60822ab8111a003a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece858afd5814f31b1311b80ca54f2ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ab13f9d5514c09b9a86c4ca799a844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "986a3e51c9bd4ee5837f978f1ed07013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1c04f1529d4195acfa5a196f86ebc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaynadimpalli/numer_ai_experiments/blob/master/My_WSB_to_Numerai_Signals_CLEANED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiq8AA2Xsx1j",
        "outputId": "da4afe72-5615-4b2a-b342-6a2583ea5ce4"
      },
      "source": [
        "!pip install praw\n",
        "!pip install vaderSentiment\n",
        "!pip install ffn\n",
        "!pip install numerapi\n",
        "\n",
        "\n",
        "import gc\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from dateutil.relativedelta import relativedelta, FR\n",
        "\n",
        "import praw #reddit data api\n",
        "import ffn #for loading financial data\n",
        "import numerapi #for numerai tickers\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #VADER sentiment model\n",
        "\n",
        "!pip install flair\n",
        "\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from flair.models.text_classification_model import TARSClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (1.1.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.7/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.26.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.26.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.0.2)\n",
            "Requirement already satisfied: ffn in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.5 in /usr/local/lib/python3.7/dist-packages (from ffn) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from ffn) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.15 in /usr/local/lib/python3.7/dist-packages (from ffn) (0.22.2.post1)\n",
            "Requirement already satisfied: future>=0.15 in /usr/local/lib/python3.7/dist-packages (from ffn) (0.16.0)\n",
            "Requirement already satisfied: matplotlib>=1 in /usr/local/lib/python3.7/dist-packages (from ffn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.15 in /usr/local/lib/python3.7/dist-packages (from ffn) (1.4.1)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.7/dist-packages (from ffn) (0.9.0)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from ffn) (0.8.9)\n",
            "Requirement already satisfied: decorator>=4 in /usr/local/lib/python3.7/dist-packages (from ffn) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1->ffn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1->ffn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1->ffn) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1->ffn) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1->ffn) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->ffn) (2018.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->ffn) (2.26.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->ffn) (4.2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->ffn) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->ffn) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->ffn) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->ffn) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15->ffn) (1.0.1)\n",
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.26.0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->numerapi) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair) (6.0.3)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.9.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.1)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair) (1.2.12)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.12)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair) (3.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWXTBbspHoI"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "o_i9soE7_HlR",
        "outputId": "226ce23b-b60e-4928-8f82-857f75a510ed"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.test.gpu_device_name() #run to make sure tensorflow is connected to gpu (if applicable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrrEFxCW-v-1"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to-nEtMn_Oo4"
      },
      "source": [
        "### Tickers we(Numerai) want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFyhCdi9_HKW",
        "outputId": "7a38dbe5-9adf-4b5e-be45-8c32ef1b4599"
      },
      "source": [
        "# Converting Bloomberg tickers to yfinance tickers\n",
        "\n",
        "napi = numerapi.SignalsAPI()\n",
        "\n",
        "eligible_tickers = pd.Series(napi.ticker_universe(), name=\"bloomberg_ticker\")\n",
        "print(f\"Number of eligible tickers : {len(eligible_tickers)}\")\n",
        "print(eligible_tickers.head(10))\n",
        "\n",
        "ticker_map = pd.read_csv(\n",
        "        'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv'\n",
        ")\n",
        "print(len(ticker_map))\n",
        "\n",
        "#Yahoo <-> Bloomberg mapping\n",
        "yfinance_tickers = eligible_tickers.map(\n",
        "        dict(zip(ticker_map[\"bloomberg_ticker\"], ticker_map[\"yahoo\"]))\n",
        "    ).dropna()\n",
        "\n",
        "bloomberg_tickers = ticker_map[\"bloomberg_ticker\"]\n",
        "print(f\"Number of eligible, mapped tickers: {len(yfinance_tickers)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of eligible tickers : 5380\n",
            "0    SVW AU\n",
            "1    GEM AU\n",
            "2    AZJ AU\n",
            "3    NXT AU\n",
            "4    TWE AU\n",
            "5    SGR AU\n",
            "6    CKF AU\n",
            "7    BGA AU\n",
            "8    QUB AU\n",
            "9    MMS AU\n",
            "Name: bloomberg_ticker, dtype: object\n",
            "5380\n",
            "Number of eligible, mapped tickers: 5328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqNgslwWsXU2"
      },
      "source": [
        "## get comments from reddit using pushshift and praw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwYGGgYHsXU2",
        "outputId": "d8ed6cff-ed8b-4d5b-eb83-5c6b7f019a24"
      },
      "source": [
        "def getPushshiftData(query, after, before, sub):\n",
        "    '''\n",
        "    Reddit Data collector from pushshift\n",
        "    '''\n",
        "\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    data = json.loads(requests.get(url).text)\n",
        "    return data['data']\n",
        "\n",
        "\n",
        "def collectSubData(subm, stats):\n",
        "    '''\n",
        "    Collect submission data and append to stats\n",
        "    '''\n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"\n",
        "\n",
        "    if flair != 'Daily Discussion':\n",
        "        return\n",
        "    \n",
        "    sub_array = [subm['id'], subm['title'], subm['url'], datetime.datetime.fromtimestamp(subm['created_utc']).date()]\n",
        "    sub_array.append(flair)\n",
        "    stats.append(sub_array)\n",
        "\n",
        "\n",
        "# Start and end dates\n",
        "date_1 = (datetime.datetime.today() + relativedelta(weekday=FR(-1)) -  datetime.timedelta(days=1 * 365)).strftime('%d/%m/%Y')\n",
        "date_2 = (datetime.datetime.today() + relativedelta(weekday=FR(-1))).strftime('%d/%m/%Y')\n",
        "print(date_1, date_2)\n",
        "\n",
        "# Converting to unix time\n",
        "date_1 = str(int(time.mktime(datetime.datetime.strptime(date_1, \"%d/%m/%Y\").timetuple()))) #july 1 2017\n",
        "date_2 = str(int(time.mktime(datetime.datetime.strptime(date_2, \"%d/%m/%Y\").timetuple()))) #july 10 2020\n",
        "\n",
        "subStats = [] # Holds submission metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/07/2020 23/07/2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoKL3NmhsXU4",
        "scrolled": true,
        "outputId": "203e1699-c619-4139-aa78-49d90f2f02f5"
      },
      "source": [
        "temp_data = []\n",
        "#Pushshift only collects 100 days of data. This loop is used to collect remaining data starting from last collected date\n",
        "while 1:\n",
        "    try:\n",
        "        temp_data = getPushshiftData(\"Daily Discussion Thread\", date_1, date_2, 'wallstreetbets')\n",
        "        # Data collected from date_1 to date_2\n",
        "        #Lazy collecting data\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if len(temp_data) == 0:\n",
        "        break\n",
        "        \n",
        "    for submission in temp_data:\n",
        "        collectSubData(submission, subStats)\n",
        "    # Calls getPushshiftData() with the created date of the last submission\n",
        "    print(len(temp_data))\n",
        "    print(str(datetime.datetime.fromtimestamp(temp_data[-1]['created_utc'])))\n",
        "    date_1 = temp_data[-1]['created_utc']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "2020-11-27 11:10:41\n",
            "100\n",
            "2021-02-11 11:00:23\n",
            "100\n",
            "2021-05-26 11:57:47\n",
            "40\n",
            "2021-07-22 10:00:18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZnUaOt-sXU4"
      },
      "source": [
        "#organize data into dataframe\n",
        "data={x:[] for x in ['id', 'title', 'url', 'date', 'flair']}\n",
        "\n",
        "for stat in subStats:\n",
        "    data['id'].append(stat[0])\n",
        "    data['title'].append(stat[1])\n",
        "    data['url'].append(stat[2])\n",
        "    data['date'].append(stat[3])\n",
        "    data['flair'].append(stat[4])\n",
        "\n",
        "df_1=pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3qyATVGYcD"
      },
      "source": [
        "## Download data from Reddit using praw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh1i82rlOl9i"
      },
      "source": [
        "[How to set-up PRAW](https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDsxRpbsXU5"
      },
      "source": [
        "#connect to reddit api\n",
        "reddit = praw.Reddit(client_id='oWl9JrNkK08efA',\n",
        "                     client_secret='6c2AbGHqSTGxNf0YIGZeV7fVlbSt3A', \n",
        "                     user_agent='CakeDayBot',\n",
        "                     username='TrailBraker_Bot', \n",
        "                     password='funracingisfun', check_for_async=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "792e07f9347f4e8db0e356a048c49b7d",
            "8c1e2c789c11433e82d1e265873cba24",
            "1f600e114d594b5e8589f563c9e6c520",
            "1a35ff6380c84e60822ab8111a003a2c",
            "ece858afd5814f31b1311b80ca54f2ac",
            "83ab13f9d5514c09b9a86c4ca799a844",
            "986a3e51c9bd4ee5837f978f1ed07013",
            "5f1c04f1529d4195acfa5a196f86ebc3"
          ]
        },
        "id": "Zj5zK56jsXU5",
        "outputId": "47773956-a0cb-4007-f79a-13c72c26ded0"
      },
      "source": [
        "#This part is the crux of the data dowloading module, here we download comments using praw\n",
        "daily_comments_text=[]\n",
        "daily_comments_upvotes = []\n",
        "for url in tqdm(df_1['url'].tolist()):\n",
        "    try:\n",
        "        reddit_submission = reddit.submission(url=url) # Dowloading submission data\n",
        "        reddit_submission.comments.replace_more(limit=0)\n",
        "        # Removing punctuations in string\n",
        "        # Using regex\n",
        "        comments=list([(re.sub(r'[^\\w\\s]', '', comment.body)) for comment in reddit_submission.comments])\n",
        "        upvotes=list([(comment.score) for comment in reddit_submission.comments])\n",
        "        \n",
        "        daily_comments_text.append(comments)\n",
        "        daily_comments_upvotes.append(upvotes)\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "792e07f9347f4e8db0e356a048c49b7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 08:52:22,365 WARNING prawcore: Retrying due to 502 status: GET https://oauth.reddit.com/comments/jku4jx/\n",
            "2021-07-28 08:55:35,157 WARNING prawcore: Retrying due to 502 status: GET https://oauth.reddit.com/comments/kxsd2p/\n",
            "2021-07-28 09:01:43,479 WARNING prawcore: Retrying due to 502 status: GET https://oauth.reddit.com/comments/nqi9f6/\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vG2EdUvGgoe"
      },
      "source": [
        "## Symbol filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7fEtl0WHj5E"
      },
      "source": [
        "Use some stop words that might create ambiguity with stock names in comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wtbKNE74riFz",
        "outputId": "cfa936f9-26c9-4559-9f05-3e754cf97b1b"
      },
      "source": [
        "#Downloading list of stopwords\n",
        "!wget https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\n",
        "\n",
        "gist_file = open(\"gist_stopwords.txt\", \"r\")\n",
        "try:\n",
        "    content = gist_file.read()\n",
        "    stop_words = content.split(\",\")\n",
        "finally:\n",
        "    gist_file.close()\n",
        "\n",
        "#Add more stop words that are used in the daily discussions\n",
        "stop_words += ['ATH', 'SAVE', 'US', 'ALL', 'LOVE', 'FOR', 'ME', \n",
        "               'GET', \"BEAT\", 'JACK', \"PUMP\", \"BIG\", \"KIDS\", 'STAY', \n",
        "               'TRUE', 'EDIT','PLAY', \"ROCK\", \"NICE\", \"DIE\", \"COST\", \n",
        "               \"WORK\", \"MF\"]\n",
        "\n",
        "stop_words = set(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-28 09:04:01--  https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6271 (6.1K) [text/plain]\n",
            "Saving to: ‘gist_stopwords.txt.1’\n",
            "\n",
            "gist_stopwords.txt. 100%[===================>]   6.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-28 09:04:01 (46.6 MB/s) - ‘gist_stopwords.txt.1’ saved [6271/6271]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ATmsBcW3jzYj",
        "outputId": "befc1cc4-e9d2-46ee-8477-9539cee28095"
      },
      "source": [
        "ticks = bloomberg_tickers.apply(lambda x: x.split(\" \")[0])\n",
        "\n",
        "# Removing ticks with len < 2\n",
        "ticks = ticks[ticks.str.len()>=2].values\n",
        "ticks = [t for t in ticks if not str.isdigit(t) and t not in stop_words and t.lower() not in stop_words]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 09:04:01,786 INFO numexpr.utils: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R0jefG_bm0MQ",
        "outputId": "e916df5b-8108-46f4-9dc8-b21a80cf5627"
      },
      "source": [
        "# ticks ban list\n",
        "# exclude_ticks = ['GME','TSLA']\n",
        "# ticks = [t for t in ticks if t not in exclude_ticks]\n",
        "\n",
        "# Checking for any intersection\n",
        "print(np.intersect1d(ticks, [s.upper() for s in stop_words]))\n",
        "\n",
        "len(ticks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3932"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M4rhRKrnJSF1"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "acFtC8CFHhuT"
      },
      "source": [
        "# # Trying this https://huggingface.co/facebook/bart-large-mnli\n",
        "\n",
        "\n",
        "# # pose sequence as a NLI premise and label as a hypothesis\n",
        "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "# nli_model = AutoModelForSequenceClassification.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
        "# tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
        "\n",
        "# premise = \"I'm gonna sell GOOG\"\n",
        "# hypothesis = 'positive'\n",
        "\n",
        "# # run through model pre-trained on MNLI\n",
        "# x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
        "#                      truncation_strategy='only_first')\n",
        "# logits = nli_model(x.to(device))[0]\n",
        "\n",
        "# # we throw away \"neutral\" (dim 1) and take the probability of\n",
        "# # \"entailment\" (2) as the probability of the label being true \n",
        "# entail_contradiction_logits = logits[:,[0,2]]\n",
        "# probs = entail_contradiction_logits.softmax(dim=1)\n",
        "# prob_label_is_true = probs[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZ0jxLEnHrG6"
      },
      "source": [
        "#Try flair here https://github.com/flairNLP/flair\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I6ahluaJsX5f",
        "outputId": "60f0c53c-cf0f-4aeb-bf64-67e21ae257ae"
      },
      "source": [
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "def flair_scorer(comment):\n",
        "\n",
        "  sentence = Sentence(comment)\n",
        "  classifier.predict(sentence)# print sentence with predicted labels\n",
        "  #TODO : Try using multi_class_prob here, result will be list of two labels instead of 1.\n",
        "  flag = 1\n",
        "  if sentence.labels[0].value == \"NEGATIVE\":\n",
        "    flag = -1\n",
        "\n",
        "  return flag * sentence.labels[0].score\n",
        "\n",
        "\n",
        "\n",
        "# 1. Load our pre-trained TARS model for English\n",
        "tars = TARSClassifier.load('tars-base')\n",
        "\n",
        "def flair_score_noshot(comment):\n",
        "  # 2. Prepare a test sentence\n",
        "  sentence = Sentence(comment)\n",
        "\n",
        "  # 3. Define some classes that you want to predict using descriptive names\n",
        "  classes = [\"positive\", \"negative\"]\n",
        "\n",
        "  #4. Predict for these classes\n",
        "  tars.predict_zero_shot(sentence, classes)\n",
        "\n",
        "  if len(sentence.labels) == 0:\n",
        "    return 0\n",
        "\n",
        "  flag = 1\n",
        "  if sentence.labels[0].value == \"negative\":\n",
        "    flag = -1\n",
        "\n",
        "  return flag * sentence.labels[0].score\n",
        "\n",
        "\n",
        "sia_analyser = SentimentIntensityAnalyzer()\n",
        "def SIA(comment):\n",
        "    sia_analyser.polarity_scores(comment)[\"compound\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 09:04:01,904 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
            "2021-07-28 09:04:11,154 loading file /root/.flair/models/tars-base-v8.pt\n",
            "init TARS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EIJed3i2zO2g",
        "outputId": "d88c5543-b7bf-405d-8e57-fbf59316db60"
      },
      "source": [
        "flair_score_noshot('Flair is pretty great!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9336798191070557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fxj7Fk45vH8T",
        "outputId": "31500a24-e8c2-4dae-c9e7-d6ac9ad62d48"
      },
      "source": [
        "flair_scorer('Flair is pretty great!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9987422823905945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZ0gWhQtrapl"
      },
      "source": [
        "def sort_list(list1, list2):\n",
        " \n",
        "    zipped_pairs = zip(list2, list1)\n",
        " \n",
        "    z = [x for _, x in sorted(zipped_pairs,reverse=True)]\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52,
          "referenced_widgets": [
            "d2f3812cc3814b55a4bb8b0db5baccf9"
          ]
        },
        "id": "7a07-RYM_0iS",
        "outputId": "7021243f-a528-4416-c60e-3612b77263d5"
      },
      "source": [
        "# Score all comments for a day based on sentiment\n",
        "# Log all tickers mentioned in those comments\n",
        "# Assign the daily sentiment to tickers involved\n",
        "\n",
        "\n",
        "## run vader sentiment analyzer\n",
        "\n",
        "sentiment_scores = [] #For entire market\n",
        "daily_tick_sentiments = [] #array of dictionaries with daily ticker sentiments\n",
        "\n",
        "for i, comments in tqdm(enumerate(daily_comments_text)):\n",
        "    # Looping through each day\n",
        "\n",
        "    sentiment_score = 0 # Initializing sentiment score each day\n",
        "    ticks_sent = {tick:0 for tick in ticks}          #daily sentiments for tickers\n",
        "    try:\n",
        "\n",
        "        # comments = sort_list(comments, daily_comments_upvotes[i])[:20]\n",
        "        # Taking top 20 comments sorted by upvotes\n",
        "\n",
        "        for j, comment in enumerate(comments):\n",
        "            if daily_comments_upvotes[i][j] <= 1: # Only taking comments with score greater than 1\n",
        "                continue\n",
        "\n",
        "            if len(comment) == 0:\n",
        "              continue\n",
        "\n",
        "            ticks_in_comment = []\n",
        "\n",
        "            for word in comment.split():\n",
        "                #Scanning for ticks mentioned in the comment\n",
        "                if word in ticks and (word.lower() not in stop_words) and (word not in stop_words):\n",
        "                    ticks_in_comment.append(word)\n",
        "\n",
        "            #comment_score = SIA(comment) #general score\n",
        "\n",
        "            #Trying out the flair models here\n",
        "            #comment_score = flair_scorer(comment)\n",
        "            comment_score = flair_score_noshot(comment)\n",
        "\n",
        "            if len(ticks_in_comment) == 1:\n",
        "              # Only using the comments with a single tick, to avoid muddying the waters.\n",
        "              for tick in ticks_in_comment:\n",
        "                  #updating the scores of comment to all ticks in the comment\n",
        "                  ticks_sent[tick] = comment_score + ticks_sent[tick]\n",
        "\n",
        "            sentiment_score = sentiment_score + comment_score\n",
        "\n",
        "        daily_tick_sentiments.append(ticks_sent) \n",
        "    except TypeError:\n",
        "        sentiment_score = 0\n",
        "\n",
        "    sentiment_scores.append(sentiment_score)\n",
        "\n",
        "df_1[\"sentiment score\"] = sentiment_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f3812cc3814b55a4bb8b0db5baccf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p69dhCQBtpgp",
        "outputId": "99fa55cd-d066-4714-de4a-688e62be99c8"
      },
      "source": [
        "daily_ticks_df = pd.concat([pd.Series(day) for day in daily_tick_sentiments], 1) # Concatenating daily ticks data to dataframe\n",
        "daily_ticks_df.columns = df_1.date.values\n",
        "\n",
        "\n",
        "# Select 50 most talked about tickers in the last 30 days\n",
        "tickers = abs(daily_ticks_df.iloc[:,-30:]).sum(1).sort_values(ascending=False)[:50].index\n",
        "# tickers that are most talked about in the last 30 days\n",
        "\n",
        "rolling_scores = daily_ticks_df.T.rolling(window=14).sum().T #Rolling sum of scores\n",
        "rolling_scores = rolling_scores.iloc[:, -7:].sum(1)\n",
        "rolling_scores = rolling_scores.loc[tickers][rolling_scores!=0]\n",
        "\n",
        "top_scores = rolling_scores.rank(pct=True).sort_values(ascending=False).reset_index()\n",
        "top_scores.columns = [\"bloomberg_ticker\", \"signal\"]\n",
        "\n",
        "\n",
        "# Mapping symbols to bloomberg symbols for numerai submission\n",
        "\n",
        "mapping = pd.Series(\n",
        "    bloomberg_tickers.values, index=bloomberg_tickers.apply(lambda x: x.split(\" \")[0])\n",
        ")\n",
        "top_scores[\"bloomberg_ticker\"] = top_scores[\"bloomberg_ticker\"].apply(\n",
        "    lambda x: mapping[x] if type(mapping[x]) == str else mapping[x].values[0]\n",
        ")\n",
        "top_scores.set_index(\"bloomberg_ticker\", inplace=True)\n",
        "top_scores.to_csv(\"Signal_SUBMISSION.csv\", index=True)\n",
        "\n",
        "print(top_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    signal\n",
            "bloomberg_ticker          \n",
            "AAPL US           1.000000\n",
            "CLF US            0.976744\n",
            "AMA AU            0.953488\n",
            "NOKIA FH          0.930233\n",
            "MSFT US           0.906977\n",
            "AMD US            0.883721\n",
            "DKNG US           0.860465\n",
            "TWTR US           0.837209\n",
            "PLUG US           0.813953\n",
            "BBBY US           0.790698\n",
            "ATT SS            0.767442\n",
            "KBH US            0.744186\n",
            "WEED CN           0.720930\n",
            "CORE US           0.697674\n",
            "BRO US            0.674419\n",
            "TSLA US           0.651163\n",
            "PYPL US           0.627907\n",
            "ABC US            0.604651\n",
            "SENS SW           0.581395\n",
            "MAN US            0.558140\n",
            "LOW US            0.534884\n",
            "NFLX US           0.511628\n",
            "VIAC US           0.488372\n",
            "CCP AU            0.465116\n",
            "AMZN US           0.441860\n",
            "LZB US            0.418605\n",
            "ASO US            0.395349\n",
            "RKT US            0.372093\n",
            "GME US            0.348837\n",
            "TEAM US           0.325581\n",
            "CAT US            0.302326\n",
            "BOX US            0.279070\n",
            "MRNA US           0.255814\n",
            "CASH US           0.232558\n",
            "FORM US           0.209302\n",
            "BB FP             0.186047\n",
            "PLTR US           0.162791\n",
            "DIS US            0.139535\n",
            "NVDA US           0.116279\n",
            "CRSR US           0.093023\n",
            "SPCE US           0.069767\n",
            "CPI LN            0.046512\n",
            "CLNE US           0.023256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKq9l7uNKv1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b9c206-d0c9-436a-e904-8fc0ef27d273"
      },
      "source": [
        "final_df = pd.read_csv(\"Signal_SUBMISSION.csv\")\n",
        "\n",
        "final_df.rename({'bloomberg_ticker':'ticker'},axis=1,inplace=True)\n",
        "\n",
        "##MAKE SURE THIS FORMAT IS CORRECT\n",
        "\n",
        "final_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ticker</th>\n",
              "      <th>signal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPL US</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CLF US</td>\n",
              "      <td>0.976744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AMA AU</td>\n",
              "      <td>0.953488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NOKIA FH</td>\n",
              "      <td>0.930233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MSFT US</td>\n",
              "      <td>0.906977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ticker    signal\n",
              "0   AAPL US  1.000000\n",
              "1    CLF US  0.976744\n",
              "2    AMA AU  0.953488\n",
              "3  NOKIA FH  0.930233\n",
              "4   MSFT US  0.906977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU_L7IWdKRuz"
      },
      "source": [
        "# public_id = \"BX5I7ZCKH3HK2U337GQSTKIPQWEBPLTQ\"\n",
        "# secret_key = \"DJK3BNVUPQNY2IAWCEHEPLEGFEXD2K6U6SOHA67F7U4U2MA7KJXED3XRAAEU4SMP\"\n",
        "# model_id = \"0fdc3056-2d36-4d04-970d-655b9f7b2021\"\n",
        "# napi = numerapi.SignalsAPI(public_id=public_id, secret_key=secret_key)\n",
        "# napi.upload_predictions(\"Signal_SUBMISSION.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nJpdwWiKutA"
      },
      "source": [
        "# # # FLAIR SUBMISSION(flair_scoreer)\n",
        "\n",
        "# public_id = \"BX5I7ZCKH3HK2U337GQSTKIPQWEBPLTQ\"\n",
        "# secret_key = \"DJK3BNVUPQNY2IAWCEHEPLEGFEXD2K6U6SOHA67F7U4U2MA7KJXED3XRAAEU4SMP\"\n",
        "# model_id = \"a0436466-72b1-415e-ae8e-1336dbe53e0a\"\n",
        "# napi = numerapi.SignalsAPI(public_id=public_id, secret_key=secret_key)\n",
        "# napi.upload_predictions(\"Signal_SUBMISSION.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YhyvM_iK7id"
      },
      "source": [
        "# FLAIR SUBMISSION(flair_scorer_noshot)\n",
        "\n",
        "# public_id = \"BX5I7ZCKH3HK2U337GQSTKIPQWEBPLTQ\"\n",
        "# secret_key = \"DJK3BNVUPQNY2IAWCEHEPLEGFEXD2K6U6SOHA67F7U4U2MA7KJXED3XRAAEU4SMP\"\n",
        "# model_id = \"cd006ccf-4f69-4bbd-9dcb-20a270ebff66\"\n",
        "# napi = numerapi.SignalsAPI(public_id=public_id, secret_key=secret_key)\n",
        "# napi.upload_predictions(\"Signal_SUBMISSION.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohK--B6y2IU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGo9lhwy2Lr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SZqgyY3-y2O4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5XjSL3VsXVF"
      },
      "source": [
        "# Plotting with S&P 500 price\n",
        "spy=ffn.get('spy', start='2010-01-01') # Downloading daily S&P price\n",
        "spy_dates=[]\n",
        "for date in tqdm(df_1['date'].astype(str).values):\n",
        "    try:\n",
        "        spy_dates.append(float(spy.loc[date]))\n",
        "    except KeyError:\n",
        "        spy_dates.append(None)\n",
        "        \n",
        "df_1['spy']=spy_dates\n",
        "\n",
        "df_plot_data=df_1[['date','sentiment score','spy']].set_index('date') # Data used for plotting\n",
        "df_plot_data=df_plot_data[df_plot_data['spy'].notna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP0PrvYasXVG"
      },
      "source": [
        "df_plot_data.plot(secondary_y='sentiment score', figsize=(16, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTRTvCEhsXVG"
      },
      "source": [
        "## Looking at fourier transforms\n",
        "\n",
        "close_fft = np.fft.fft(np.asarray(df_plot_data['sentiment score'].tolist())) # FFT on the sentiment score\n",
        "fft_df = pd.DataFrame({'fft':close_fft})\n",
        "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
        "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
        "fft_list = np.asarray(fft_df['fft'].tolist())\n",
        "\n",
        "for num_ in [5, 10, 15, 20]:\n",
        "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
        "    df_plot_data['fourier '+str(num_)]=np.fft.ifft(fft_list_m10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flparmbPsXVH"
      },
      "source": [
        "df_plot_data[['sentiment score', 'fourier 5', 'fourier 10', 'fourier 15', 'fourier 20']].plot(figsize=(16, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mTbkLTTsXVH"
      },
      "source": [
        "df_plot_data[['spy', 'fourier 20']].plot(secondary_y='fourier 20', figsize=(16, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGn-IJBsXVI"
      },
      "source": [
        "#normalize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc= MinMaxScaler(feature_range=(0,1))\n",
        "df_plot_data['norm_price']=sc.fit_transform(df_plot_data['spy'].to_numpy().reshape(-1, 1))\n",
        "df_plot_data['spy log']=np.log(df_plot_data['spy']/df_plot_data['spy'].shift(1))\n",
        "df_plot_data['norm_sentiment']=sc.fit_transform(df_plot_data['sentiment score'].to_numpy().reshape(-1, 1))\n",
        "df_plot_data['norm_fourier5']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_plot_data['fourier 5'].to_numpy()])).reshape(-1, 1))\n",
        "df_plot_data['norm_fourier10']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_plot_data['fourier 10'].to_numpy()])).reshape(-1, 1))\n",
        "df_plot_data['norm_fourier15']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_plot_data['fourier 15'].to_numpy()])).reshape(-1, 1))\n",
        "df_plot_data['norm_fourier20']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_plot_data['fourier 20'].to_numpy()])).reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxvnAgbssXVI"
      },
      "source": [
        "df_plot_data[['norm_price', 'norm_sentiment', 'norm_fourier5', 'norm_fourier20']].plot(figsize=(16, 10));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QoS8k-1_J8e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR3N8ZDKKRXD"
      },
      "source": [
        "---"
      ]
    }
  ]
}