{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Numerai Trials",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaynadimpalli/numer_ai_experiments/blob/master/Numerai_Trials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3mZNUtg1Bbp"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKQ_zqwTBf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b734d933-247b-47c0-8081-78bf87b9d9fb"
      },
      "source": [
        "!git clone https://github.com/vijaynadimpalli/numer_ai_experiments.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'numer_ai_experiments'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 10 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsnHuxV9oNGD"
      },
      "source": [
        "from numer_ai_experiments.utils import *\n",
        "from numer_ai_experiments.tf_classes import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaNpcfNkKUg_"
      },
      "source": [
        "# install dependencies\n",
        "try:\n",
        "  import numerapi\n",
        "except:\n",
        "  !pip install numerapi\n",
        "  import numerapi\n",
        " \n",
        " \n",
        "# import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc,os\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Dense,Dropout,BatchNormalization,Activation,LSTM,Lambda,Bidirectional,GaussianNoise,Concatenate\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,LogCosh\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.activations import swish\n",
        " \n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        " \n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split,GroupKFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        " \n",
        "import random\n",
        "def set_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EyB8NsbUNNWR",
        "outputId": "7e1401cd-fb59-4530-b569-6bcde654cdb4"
      },
      "source": [
        "training_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\",nrows=1)\n",
        "dtype_dict = {x:np.float32 for x in training_data.columns if ('feature' in x or 'target' in x)}\n",
        "#Not using float16 because np.linalg in _neutralize function does not support it.\n",
        "\n",
        "training_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\",dtype=dtype_dict)\n",
        "\n",
        "\n",
        "#Centering all features and target data around 0 (for good NN convergence)\n",
        "training_data['target'] -= 0.5\n",
        "\n",
        "#Using .loc and mask for below operation uses a lot of  memory \n",
        "for col in (x for x in training_data.columns if 'feature' in x):\n",
        "  training_data[col] -= 0.5\n",
        "\n",
        "#data is float64 now...\n",
        "\n",
        "#training_data = training_data.astype(dtype_dict)\n",
        "#converting training data to float32 seems to have decreased the correlation from 427 to 407 to 380...\n",
        "\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id   era data_type  ...  feature_wisdom45  feature_wisdom46  target\n",
              "0  n000315175b67977  era1     train  ...              0.00              0.25    0.00\n",
              "1  n0014af834a96cdd  era1     train  ...             -0.25              0.50   -0.25\n",
              "2  n001c93979ac41d4  era1     train  ...             -0.25              0.25   -0.25\n",
              "3  n0034e4143f22a13  era1     train  ...              0.50              0.50   -0.25\n",
              "4  n00679d1a636062f  era1     train  ...             -0.25              0.25    0.25\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M9Y-litu5qVO",
        "outputId": "2c069014-06c3-46ed-fd24-7079bfe5f9d8"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3624"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OppRwr3MRz4g"
      },
      "source": [
        "df = training_data\n",
        "#120 eras\n",
        "#Values of all features and targets are either 0,0.25,0.5,0.75,1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bcw2olpgR5tk"
      },
      "source": [
        "# train_features = df[[x for x in df.columns if 'feature' in x]]\n",
        "# train_targets = df[['target']]\n",
        "\n",
        "# valid_features = tournament_data.query(\"data_type == 'validation'\")[[x for x in df.columns if 'feature' in x]]\n",
        "# valid_targets = tournament_data.query(\"data_type == 'validation'\")[['target']]\n",
        "#features.columns\n",
        "\n",
        "X = df[[x for x in df.columns if 'feature' in x]].values\n",
        "y = df[['target']].values\n",
        "\n",
        "features = [x for x in df.columns if 'feature' in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzFztoHGXlHg"
      },
      "source": [
        "# Tensorflow autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnaad3Gt82LF"
      },
      "source": [
        "def create_autoencoder(input_dim,output_dim,noise=0.09):\n",
        "    i = Input(input_dim)\n",
        "    encoded = BatchNormalization()(i)\n",
        "    encoded = GaussianNoise(noise)(encoded)\n",
        "    encoded = Dense(128,activation='relu')(encoded)\n",
        " \n",
        "    decoded = Dropout(0.2)(encoded)\n",
        "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
        " \n",
        "    x = Dense(128,activation='relu')(decoded)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(32,activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(output_dim,activation='linear',name='label_output')(x)\n",
        "    \n",
        "    encoder = Model(inputs=i,outputs=encoded)\n",
        "    autoencoder = Model(inputs=i,outputs=[decoded,x])        \n",
        "    return autoencoder, encoder\n",
        " \n",
        " \n",
        " \n",
        "def create_model(input_shape,output_shape,params,encoder):\n",
        "    inp = Input(input_shape)\n",
        "    \n",
        "    X = encoder(inp)\n",
        "    X = Concatenate()([X,inp])\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(params['dropout'][0])(X)\n",
        "    \n",
        "    for layer in range(len(params['units'])):\n",
        "        X = Dense(params['units'][layer])(X)\n",
        "        X = BatchNormalization()(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Dropout(params['dropout'][layer + 1])(X)\n",
        "        \n",
        "    out = Dense(output_shape,activation = 'linear')(X)\n",
        "    \n",
        "    return Model(inputs=[inp],outputs=[out])\n",
        " \n",
        " \n",
        "def run_model(X_train, X_test, y_train, y_test,params,encoder,callbacks=[]): \n",
        " \n",
        "    model = create_model(X_train.shape[-1],y_train.shape[-1],params,encoder)\n",
        "    #print(model.summary())    \n",
        "    \n",
        "    model.compile(optimizer=Adam(learning_rate = params['lr']),loss=LogCosh(),metrics=[tf_correlation],\n",
        "                  #run_eagerly=True\n",
        "                  )\n",
        "    \n",
        "    model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callbacks,epochs=params['epochs'],batch_size=params['batch_size'],verbose=params['verbose'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "2QH3TP8oA8h8",
        "outputId": "defbf85a-8917-422e-ff97-384e2696aac7"
      },
      "source": [
        "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
        "set_all_seeds(42)\n",
        "\n",
        "\n",
        "autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'mse'})\n",
        "print(autoencoder.summary())  \n",
        "autoencoder.fit(X,(X,y),\n",
        "                epochs=1000,\n",
        "                batch_size=4096, \n",
        "                validation_split=0.1,\n",
        "                callbacks=[EarlyStopping('val_loss',patience=30,restore_best_weights=True)],verbose=0)\n",
        "encoder.save_weights('./encoder.hdf5')\n",
        "encoder.trainable = False\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-138021989f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset_all_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'decoded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label_output'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ce_Ga0MbCrVK"
      },
      "source": [
        "gc.collect()\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S_-vCA28sVqw"
      },
      "source": [
        "#Code for baseline model without using hyperopt\n",
        "\n",
        "# #1 iteration 830 seconds on tesla T4\n",
        "# #final_score 0.022 for initial lr 1e-3\n",
        "# #final_score 0.0123 for intial lr 1e-4\n",
        "\n",
        "# best_hopt={\n",
        "#     'dropout_0': 0.13133861210545011,\n",
        "#  'dropout_1': 0.27060268015898126,\n",
        "#  'dropout_2': 0.4889096286518324,\n",
        "#  'dropout_3': 0.29340634238215346,\n",
        "#  'dropout_4': 0.49916195991607193,\n",
        "#  'units_1': 896,\n",
        "#  'units_2': 512,\n",
        "#  'units_3': 512,\n",
        "#  'units_4': 896}\n",
        "\n",
        "\n",
        "# para = best_hopt\n",
        "# print(para)\n",
        "# params = {}\n",
        "# params['batch_size'] = 4096\n",
        "# params['lr'] = 1e-3\n",
        "# params['units'] = [para[x] for x in list(para.keys()) if 'units' in x]\n",
        "# params['dropout'] = [para[x] for x in list(para.keys()) if 'dropout' in x]\n",
        "# params['epochs'] = 100\n",
        "# params['label_smoothing'] = 1e-2\n",
        "# params['verbose'] = 2\n",
        "\n",
        "\n",
        "# set_all_seeds(42)\n",
        "\n",
        "# gkfold = GroupKFold(5)\n",
        "\n",
        "\n",
        "# scores = np.zeros(5)\n",
        "# feature_expo_folds = []\n",
        "\n",
        "# for iter,(train_index,test_index) in enumerate(gkfold.split(X,y,training_data['era'])):\n",
        "#     X_train, y_train = X[train_index], y[train_index]\n",
        "#     X_test, y_test = X[test_index], y[test_index]\n",
        "\n",
        "#     df2 = df.iloc[test_index,:].copy()    \n",
        "\n",
        "#     # imp = SimpleImputer(strategy='mean',copy=True,verbose=1)\n",
        "#     # X_train = imp.fit_transform(X_train)\n",
        "#     # X_test = imp.transform(X_test)\n",
        "#     ck_path = f\"model_{iter}.h5\"\n",
        "    \n",
        "#     ckpr = ModelCheckpoint(ck_path,monitor='val_tf_correlation',save_best_only=True,save_weights_only=True,mode='max',verbose=1)\n",
        "#     early = EarlyStopping(monitor='val_tf_correlation',min_delta=1e-5,mode='max',patience=10,restore_best_weights=False,verbose=1)\n",
        "#     rlr = ReduceLROnPlateau(monitor = 'val_tf_correlation', factor = 0.1, patience = 4, verbose = 1, min_delta = 1e-5, mode = 'max') \n",
        "#     #Bug in restore_best_weights https://github.com/keras-team/keras/issues/12511 \n",
        "#     model = run_model(X_train, X_test, y_train, y_test,params,encoder,callbacks=[ckpr,early,rlr])\n",
        "#     model.load_weights(ck_path)\n",
        "#     y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "#     df2[\"preds\"] = y_pred\n",
        "#     df2[\"preds_neutralized\"] = df2.groupby(\"era\").apply(\n",
        "#     lambda x: normalize_and_neutralize(x, [\"preds\"], features, 0.5) # neutralize by 50% within each era\n",
        "#     )\n",
        "#     scaler = MinMaxScaler()\n",
        "#     df2[\"preds_neutralized\"] = scaler.fit_transform(df2[[\"preds_neutralized\"]]) # transform back to 0-1\n",
        "\n",
        "#     # unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds\"])[0,1])\n",
        "#     # balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds_neutralized\"])[0,1])\n",
        "\n",
        "#     spearman_unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds\"]))\n",
        "#     spearman_balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds_neutralized\"]))\n",
        "\n",
        "#     # print(f\"score for high feature exposure: {unbalanced_scores_per_era.mean()}\")\n",
        "#     # print(f\"score for balanced feature expo: {balanced_scores_per_era.mean()}\")\n",
        "#     print(f\"Spearman score for high feature exposure: {spearman_unbalanced_scores_per_era.mean()}\")\n",
        "#     print(f\"Spearman score for balanced feature expo: {spearman_balanced_scores_per_era.mean()}\")\n",
        "\n",
        "#     print(f\"std for high feature exposure: {spearman_unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "#     print(f\"std for balanced feature expo: {spearman_balanced_scores_per_era.std(ddof=0)}\")\n",
        "\n",
        "#     try:\n",
        "#       print(f\"smart sharpe for high feature exposure: {smart_sharpe(spearman_unbalanced_scores_per_era)}\")\n",
        "#       print(f\"smart sharpe for balanced feature expo: {smart_sharpe(spearman_balanced_scores_per_era)}\")\n",
        "#     except ZeroDivisionError:\n",
        "#       print(\"Division by zero!!!\")\n",
        "#       pass\n",
        "\n",
        "#     try:\n",
        "#       print(f\"smart sortino for high feature exposure: {smart_sortino_ratio(spearman_unbalanced_scores_per_era)}\")\n",
        "#       print(f\"smart sortino for balanced feature expo: {smart_sortino_ratio(spearman_balanced_scores_per_era)}\")\n",
        "#     except ZeroDivisionError:\n",
        "#       print(\"Division by zero!!!\")\n",
        "#       pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     #Calculating feature exposures...\n",
        "#     # feat_expo = []\n",
        "#     # for feature in features:\n",
        "#     #     feat_expo.append(np.corrcoef(df2[feature], df2[\"preds_neutralized\"])[0,1])\n",
        "#     # feat_expo = pd.Series(feat_expo, index=features)\n",
        "\n",
        "#     # feature_expo_folds.append(feat_expo)\n",
        "\n",
        "    \n",
        "\n",
        "#     scores[iter] = spearman_unbalanced_scores_per_era.mean()\n",
        "\n",
        "#     #Transfer Learning...\n",
        "#     # p = params.copy()\n",
        "#     # p['lr'] = p['lr']/100\n",
        "#     # p['epochs'] = 3\n",
        "#     # model = create_model(X_test.shape[-1],y_test.shape[-1],p,encoder)\n",
        "#     # model.compile(optimizer=Adam(learning_rate = p['lr']),loss=BinaryCrossentropy(label_smoothing=p['label_smoothing']),metrics=[AUC(name='AUC')])\n",
        "#     # model.load_weights(ck_path)\n",
        "#     # model.fit(X_test,y_test,epochs=p['epochs'],batch_size=p['batch_size'],verbose=p['verbose'])\n",
        "#     # model.save_weights(ck_path)\n",
        "\n",
        "#     K.clear_session()\n",
        "#     del model,X_train, y_train,X_test, y_test,y_pred, df2\n",
        "#     rubbish = gc.collect()\n",
        "#     #print('Memory used: ' + str(psutil.virtual_memory().used // 1e6))\n",
        "    \n",
        "    \n",
        "# for iter,score in enumerate(scores):\n",
        "#     print(f\"Score for fold {iter+1} is : {score}\")\n",
        "\n",
        "# final_score = scores.mean()\n",
        "# print(f\"Final Score is : {final_score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bNarTz0iZ6tG"
      },
      "source": [
        "#feature_expo_folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP3RrCRGGZX3"
      },
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials,space_eval\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "gc.collect()\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7kHLho6dFFxn"
      },
      "source": [
        "# def optimise(para):\n",
        "#   print(para)\n",
        "#   params = {}\n",
        "#   params['batch_size'] = 4096\n",
        "#   params['lr'] = 1e-3\n",
        "#   params['units'] = para['units'][1:]\n",
        "#   params['dropout'] = para['dropout']\n",
        "#   params['epochs'] = 100\n",
        "#   params['label_smoothing'] = 1e-2\n",
        "#   params['verbose'] = 0\n",
        "\n",
        "\n",
        "#   set_all_seeds(42)\n",
        "#   gkfold = GroupKFold(5)\n",
        "#   scores = np.zeros(5)\n",
        "#   feature_expo_folds = []\n",
        "\n",
        "#   for iter,(train_index,test_index) in enumerate(gkfold.split(X,y,training_data['era'])):\n",
        "#     X_train, y_train = X[train_index], y[train_index]\n",
        "#     X_test, y_test = X[test_index], y[test_index]\n",
        "\n",
        "#     df2 = df.iloc[test_index,:].copy()    \n",
        "\n",
        "#     # imp = SimpleImputer(strategy='mean',copy=True,verbose=1)\n",
        "#     # X_train = imp.fit_transform(X_train)\n",
        "#     # X_test = imp.transform(X_test)\n",
        "#     ck_path = f\"model_{iter}.h5\"\n",
        "    \n",
        "#     ckpr = ModelCheckpoint(ck_path,monitor='val_tf_correlation',save_best_only=True,save_weights_only=True,mode='max',verbose=1)\n",
        "#     early = EarlyStopping(monitor='val_tf_correlation',min_delta=1e-5,mode='max',patience=10,restore_best_weights=False,verbose=1)\n",
        "#     rlr = ReduceLROnPlateau(monitor = 'val_tf_correlation', factor = 0.1, patience = 4, verbose = 1, min_delta = 1e-5, mode = 'max') \n",
        "#     #Bug in restore_best_weights https://github.com/keras-team/keras/issues/12511 \n",
        "#     model = run_model(X_train, X_test, y_train, y_test,params,encoder,callbacks=[ckpr,early,rlr])\n",
        "#     model.load_weights(ck_path)\n",
        "#     y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "#     df2[\"preds\"] = y_pred\n",
        "#     df2[\"preds_neutralized\"] = df2.groupby(\"era\").apply(\n",
        "#     lambda x: normalize_and_neutralize(x, [\"preds\"], features, 0.5) # neutralize by 50% within each era\n",
        "#     )\n",
        "#     scaler = MinMaxScaler()\n",
        "#     df2[\"preds_neutralized\"] = scaler.fit_transform(df2[[\"preds_neutralized\"]]) # transform back to 0-1\n",
        "\n",
        "#     # unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds\"])[0,1])\n",
        "#     # balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds_neutralized\"])[0,1])\n",
        "\n",
        "#     spearman_unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds\"]))\n",
        "#     spearman_balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds_neutralized\"]))\n",
        "\n",
        "#     # print(f\"score for high feature exposure: {unbalanced_scores_per_era.mean()}\")\n",
        "#     # print(f\"score for balanced feature expo: {balanced_scores_per_era.mean()}\")\n",
        "#     print(f\"Spearman score for high feature exposure: {spearman_unbalanced_scores_per_era.mean()}\")\n",
        "#     print(f\"Spearman score for balanced feature expo: {spearman_balanced_scores_per_era.mean()}\")\n",
        "\n",
        "#     print(f\"std for high feature exposure: {spearman_unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "#     print(f\"std for balanced feature expo: {spearman_balanced_scores_per_era.std(ddof=0)}\")\n",
        "\n",
        "#     try:\n",
        "#       print(f\"smart sharpe for high feature exposure: {smart_sharpe(spearman_unbalanced_scores_per_era)}\")\n",
        "#       print(f\"smart sharpe for balanced feature expo: {smart_sharpe(spearman_balanced_scores_per_era)}\")\n",
        "#     except ZeroDivisionError:\n",
        "#       print(\"Division by zero!!!\")\n",
        "#       pass\n",
        "\n",
        "#     try:\n",
        "#       print(f\"smart sortino for high feature exposure: {smart_sortino_ratio(spearman_unbalanced_scores_per_era)}\")\n",
        "#       print(f\"smart sortino for balanced feature expo: {smart_sortino_ratio(spearman_balanced_scores_per_era)}\")\n",
        "#     except ZeroDivisionError:\n",
        "#       print(\"Division by zero!!!\")\n",
        "#       pass\n",
        "\n",
        "\n",
        "#     #Calculating feature exposures...\n",
        "#     # feat_expo = []\n",
        "#     # for feature in features:\n",
        "#     #     feat_expo.append(np.corrcoef(df2[feature], df2[\"preds_neutralized\"])[0,1])\n",
        "#     # feat_expo = pd.Series(feat_expo, index=features)\n",
        "\n",
        "#     # feature_expo_folds.append(feat_expo)    \n",
        "\n",
        "#     scores[iter] = smart_sharpe(spearman_balanced_scores_per_era)\n",
        "\n",
        "#     K.clear_session()\n",
        "#     del model,X_train, y_train,X_test, y_test,y_pred, df2\n",
        "#     rubbish = gc.collect()\n",
        "#     #print('Memory used: ' + str(psutil.virtual_memory().used // 1e6))\n",
        "      \n",
        "      \n",
        "#   for iter,score in enumerate(scores):\n",
        "#     print(f\"Score for fold {iter+1} is : {score}\")\n",
        "\n",
        "#   final_score = scores.mean()\n",
        "#   print(f\"Final Score is : {final_score}\")\n",
        "#   return -final_score    \n",
        "    \n",
        "\n",
        "\n",
        "# param_space = {}\n",
        "\n",
        "# num_layers = 5\n",
        "# param_space['units'] = [None]*num_layers\n",
        "# param_space['dropout'] = [None]*num_layers\n",
        "# param_space['dropout'][0] = hp.uniform('dropout_0',0.1,0.5)\n",
        "\n",
        "# for ind in np.arange(1,num_layers):\n",
        "#   param_space['units'][ind] = hp.choice(f'units_{ind}',np.arange(128,1024,128))\n",
        "#   param_space['dropout'][ind] = hp.uniform(f'dropout_{ind}',0.1,0.5)\n",
        "\n",
        "# trials = Trials()\n",
        "\n",
        "# hopt = fmin(fn = optimise, \n",
        "#             space = param_space, \n",
        "#             algo = tpe.suggest, \n",
        "#             max_evals = 50, \n",
        "#             trials = trials, \n",
        "#            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "c-5wmhClFIk5",
        "outputId": "ff7ed4fd-55a6-40bf-ef6d-ddef80a9ee66"
      },
      "source": [
        "# best_hopt = space_eval(param_space, hopt)\n",
        "# best_hopt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#HP V1\n",
        "#validation scores...\n",
        "# Spearman score for high feature exposure: 0.02024075585765478\n",
        "# Spearman score for balanced feature expo: 0.020506771934733794\n",
        "# std for high feature exposure: 0.03409353952777108\n",
        "# std for balanced feature expo: 0.030208522173948052\n",
        "# smart sharpe for high feature exposure: nan\n",
        "# smart sharpe for balanced feature expo: nan\n",
        "\"\"\"\n",
        "\n",
        "scores[iter] = spearman_unbalanced_scores_per_era.mean()\n",
        "\n",
        "\n",
        "best loss: -0.04273940296590138\n",
        "\n",
        "{'dropout': (0.10847936278162261,\n",
        "  0.19700177820801823,\n",
        "  0.289236771380973,\n",
        "  0.41188042009646997,\n",
        "  0.1978840381104568),\n",
        " 'units': (None, 512, 768, 384, 896)}\n",
        " \"\"\"\n",
        "\n",
        "#HP V2\n",
        "#validation scores...\n",
        "# Spearman score for high feature exposure: 0.021036872796273743\n",
        "# Spearman score for balanced feature expo: 0.021122382375489433\n",
        "# std for high feature exposure: 0.028269333087386668\n",
        "# std for balanced feature expo: 0.02364757360596254\n",
        "# smart sharpe for high feature exposure: nan\n",
        "# smart sharpe for balanced feature expo: nan\n",
        "\"\"\"\n",
        "\n",
        "  scores[iter] = smart_sharpe(spearman_balanced_scores_per_era)\n",
        "  #Taking the smart sharpe ratio on neutralised data as score for hyperopt to optimize\n",
        "\n",
        "  best loss: -1.434694787023735\n",
        "\n",
        "    {'dropout': (0.18390444344120305,\n",
        "  0.3011358381508519,\n",
        "  0.10337490449824736,\n",
        "  0.1021643294765179,\n",
        "  0.16387070003988394),\n",
        " 'units': (None, 896, 896, 512, 896)}\n",
        "\n",
        " \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n  scores[iter] = smart_sharpe(spearman_balanced_scores_per_era)\\n  #Taking the smart sharpe ratio on neutralised data as score for hyperopt to optimize\\n\\n  best loss: -1.434694787023735\\n\\n    {'dropout': (0.18390444344120305,\\n  0.3011358381508519,\\n  0.10337490449824736,\\n  0.1021643294765179,\\n  0.16387070003988394),\\n 'units': (None, 896, 896, 512, 896)}\\n\\n \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53AkvIOgKOup",
        "outputId": "896fc3bd-9e00-434a-e8e0-68d207b28b5a"
      },
      "source": [
        "#Using HP V2\n",
        "best_hopt= {'dropout': (0.18390444344120305,\n",
        "  0.3011358381508519,\n",
        "  0.10337490449824736,\n",
        "  0.1021643294765179,\n",
        "  0.16387070003988394),\n",
        " 'units': (None, 896, 896, 512, 896)}\n",
        " \n",
        " \n",
        "para = best_hopt\n",
        "print(para)\n",
        "params = {}\n",
        "params['batch_size'] = 4096\n",
        "params['lr'] = 1e-3\n",
        "params['units'] = para['units'][1:]\n",
        "params['dropout'] = para['dropout']\n",
        "params['epochs'] = 100\n",
        "params['label_smoothing'] = 1e-2\n",
        "params['verbose'] = 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dropout': (0.18390444344120305, 0.3011358381508519, 0.10337490449824736, 0.1021643294765179, 0.16387070003988394), 'units': (None, 896, 896, 512, 896)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6HMuSJELDI",
        "outputId": "7f64d387-ac73-4ed9-a047-9b0ea581a830"
      },
      "source": [
        "set_all_seeds(42)\n",
        "\n",
        "gkfold = GroupKFold(5)\n",
        "\n",
        "\n",
        "scores = np.zeros(5)\n",
        "feature_expo_folds = []\n",
        "\n",
        "for iter,(train_index,test_index) in enumerate(gkfold.split(X,y,training_data['era'])):\n",
        "    X_train, y_train = X[train_index], y[train_index]\n",
        "    X_test, y_test = X[test_index], y[test_index]\n",
        "\n",
        "    df2 = df.iloc[test_index,:].copy()    \n",
        "\n",
        "    # imp = SimpleImputer(strategy='mean',copy=True,verbose=1)\n",
        "    # X_train = imp.fit_transform(X_train)\n",
        "    # X_test = imp.transform(X_test)\n",
        "    ck_path = f\"model_{iter}.h5\"\n",
        "    \n",
        "    ckpr = ModelCheckpoint(ck_path,monitor='val_tf_correlation',save_best_only=True,save_weights_only=True,mode='max',verbose=1)\n",
        "    early = EarlyStopping(monitor='val_tf_correlation',min_delta=1e-5,mode='max',patience=10,restore_best_weights=False,verbose=1)\n",
        "    rlr = ReduceLROnPlateau(monitor = 'val_tf_correlation', factor = 0.1, patience = 4, verbose = 1, min_delta = 1e-5, mode = 'max') \n",
        "    #Bug in restore_best_weights https://github.com/keras-team/keras/issues/12511 \n",
        "    model = run_model(X_train, X_test, y_train, y_test,params,encoder,callbacks=[ckpr,early,rlr])\n",
        "    model.load_weights(ck_path)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "    df2[\"preds\"] = y_pred\n",
        "    df2[\"preds_neutralized\"] = df2.groupby(\"era\").apply(\n",
        "    lambda x: normalize_and_neutralize(x, [\"preds\"], features, 0.5) # neutralize by 50% within each era\n",
        "    )\n",
        "    scaler = MinMaxScaler()\n",
        "    df2[\"preds_neutralized\"] = scaler.fit_transform(df2[[\"preds_neutralized\"]]) # transform back to 0-1\n",
        "\n",
        "    # unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds\"])[0,1])\n",
        "    # balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"target\"], d[\"preds_neutralized\"])[0,1])\n",
        "\n",
        "    spearman_unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds\"]))\n",
        "    spearman_balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds_neutralized\"]))\n",
        "\n",
        "    # print(f\"score for high feature exposure: {unbalanced_scores_per_era.mean()}\")\n",
        "    # print(f\"score for balanced feature expo: {balanced_scores_per_era.mean()}\")\n",
        "    print(f\"Spearman score for high feature exposure: {spearman_unbalanced_scores_per_era.mean()}\")\n",
        "    print(f\"Spearman score for balanced feature expo: {spearman_balanced_scores_per_era.mean()}\")\n",
        "\n",
        "    print(f\"std for high feature exposure: {spearman_unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "    print(f\"std for balanced feature expo: {spearman_balanced_scores_per_era.std(ddof=0)}\")\n",
        "\n",
        "    try:\n",
        "      print(f\"smart sharpe for high feature exposure: {smart_sharpe(spearman_unbalanced_scores_per_era)}\")\n",
        "      print(f\"smart sharpe for balanced feature expo: {smart_sharpe(spearman_balanced_scores_per_era)}\")\n",
        "    except ZeroDivisionError:\n",
        "      print(\"Division by zero!!!\")\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      print(f\"smart sortino for high feature exposure: {smart_sortino_ratio(spearman_unbalanced_scores_per_era)}\")\n",
        "      print(f\"smart sortino for balanced feature expo: {smart_sortino_ratio(spearman_balanced_scores_per_era)}\")\n",
        "    except ZeroDivisionError:\n",
        "      print(\"Division by zero!!!\")\n",
        "      pass\n",
        "\n",
        "\n",
        "    #Calculating feature exposures...\n",
        "    # feat_expo = []\n",
        "    # for feature in features:\n",
        "    #     feat_expo.append(np.corrcoef(df2[feature], df2[\"preds_neutralized\"])[0,1])\n",
        "    # feat_expo = pd.Series(feat_expo, index=features)\n",
        "\n",
        "    # feature_expo_folds.append(feat_expo)\n",
        "\n",
        "    \n",
        "\n",
        "    scores[iter] = spearman_unbalanced_scores_per_era.mean()\n",
        "\n",
        "    #Transfer Learning...\n",
        "    # p = params.copy()\n",
        "    # p['lr'] = p['lr']/100\n",
        "    # p['epochs'] = 3\n",
        "    # model = create_model(X_test.shape[-1],y_test.shape[-1],p,encoder)\n",
        "    # model.compile(optimizer=Adam(learning_rate = p['lr']),loss=BinaryCrossentropy(label_smoothing=p['label_smoothing']),metrics=[AUC(name='AUC')])\n",
        "    # model.load_weights(ck_path)\n",
        "    # model.fit(X_test,y_test,epochs=p['epochs'],batch_size=p['batch_size'],verbose=p['verbose'])\n",
        "    # model.save_weights(ck_path)\n",
        "\n",
        "    K.clear_session()\n",
        "    del model,X_train, y_train,X_test, y_test,y_pred, df2\n",
        "    rubbish = gc.collect()\n",
        "    #print('Memory used: ' + str(psutil.virtual_memory().used // 1e6))\n",
        "    \n",
        "    \n",
        "for iter,score in enumerate(scores):\n",
        "    print(f\"Score for fold {iter+1} is : {score}\")\n",
        "\n",
        "final_score = scores.mean()\n",
        "print(f\"Final Score is : {final_score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "98/98 - 5s - loss: 0.1237 - tf_correlation: -3.4254e-04 - val_loss: 0.0270 - val_tf_correlation: -5.2503e-03\n",
            "\n",
            "Epoch 00001: val_tf_correlation improved from -inf to -0.00525, saving model to model_0.h5\n",
            "Epoch 2/100\n",
            "98/98 - 3s - loss: 0.0435 - tf_correlation: 0.0018 - val_loss: 0.0249 - val_tf_correlation: 0.0085\n",
            "\n",
            "Epoch 00002: val_tf_correlation improved from -0.00525 to 0.00850, saving model to model_0.h5\n",
            "Epoch 3/100\n",
            "98/98 - 3s - loss: 0.0305 - tf_correlation: 6.6012e-04 - val_loss: 0.0245 - val_tf_correlation: 0.0143\n",
            "\n",
            "Epoch 00003: val_tf_correlation improved from 0.00850 to 0.01432, saving model to model_0.h5\n",
            "Epoch 4/100\n",
            "98/98 - 3s - loss: 0.0267 - tf_correlation: 0.0043 - val_loss: 0.0243 - val_tf_correlation: 0.0199\n",
            "\n",
            "Epoch 00004: val_tf_correlation improved from 0.01432 to 0.01990, saving model to model_0.h5\n",
            "Epoch 5/100\n",
            "98/98 - 3s - loss: 0.0255 - tf_correlation: 0.0066 - val_loss: 0.0246 - val_tf_correlation: 0.0192\n",
            "\n",
            "Epoch 00005: val_tf_correlation did not improve from 0.01990\n",
            "Epoch 6/100\n",
            "98/98 - 3s - loss: 0.0249 - tf_correlation: 0.0094 - val_loss: 0.0244 - val_tf_correlation: 0.0156\n",
            "\n",
            "Epoch 00006: val_tf_correlation did not improve from 0.01990\n",
            "Epoch 7/100\n",
            "98/98 - 3s - loss: 0.0247 - tf_correlation: 0.0112 - val_loss: 0.0243 - val_tf_correlation: 0.0179\n",
            "\n",
            "Epoch 00007: val_tf_correlation did not improve from 0.01990\n",
            "Epoch 8/100\n",
            "98/98 - 3s - loss: 0.0247 - tf_correlation: 0.0133 - val_loss: 0.0244 - val_tf_correlation: 0.0216\n",
            "\n",
            "Epoch 00008: val_tf_correlation improved from 0.01990 to 0.02156, saving model to model_0.h5\n",
            "Epoch 9/100\n",
            "98/98 - 3s - loss: 0.0245 - tf_correlation: 0.0174 - val_loss: 0.0244 - val_tf_correlation: 0.0240\n",
            "\n",
            "Epoch 00009: val_tf_correlation improved from 0.02156 to 0.02397, saving model to model_0.h5\n",
            "Epoch 10/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0211 - val_loss: 0.0243 - val_tf_correlation: 0.0244\n",
            "\n",
            "Epoch 00010: val_tf_correlation improved from 0.02397 to 0.02444, saving model to model_0.h5\n",
            "Epoch 11/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0244 - val_loss: 0.0243 - val_tf_correlation: 0.0279\n",
            "\n",
            "Epoch 00011: val_tf_correlation improved from 0.02444 to 0.02786, saving model to model_0.h5\n",
            "Epoch 12/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0314 - val_loss: 0.0243 - val_tf_correlation: 0.0271\n",
            "\n",
            "Epoch 00012: val_tf_correlation did not improve from 0.02786\n",
            "Epoch 13/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0325 - val_loss: 0.0243 - val_tf_correlation: 0.0332\n",
            "\n",
            "Epoch 00013: val_tf_correlation improved from 0.02786 to 0.03319, saving model to model_0.h5\n",
            "Epoch 14/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0346 - val_loss: 0.0245 - val_tf_correlation: 0.0387\n",
            "\n",
            "Epoch 00014: val_tf_correlation improved from 0.03319 to 0.03875, saving model to model_0.h5\n",
            "Epoch 15/100\n",
            "98/98 - 3s - loss: 0.0245 - tf_correlation: 0.0346 - val_loss: 0.0245 - val_tf_correlation: 0.0316\n",
            "\n",
            "Epoch 00015: val_tf_correlation did not improve from 0.03875\n",
            "Epoch 16/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0389 - val_loss: 0.0244 - val_tf_correlation: 0.0318\n",
            "\n",
            "Epoch 00016: val_tf_correlation did not improve from 0.03875\n",
            "Epoch 17/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0384 - val_loss: 0.0243 - val_tf_correlation: 0.0344\n",
            "\n",
            "Epoch 00017: val_tf_correlation did not improve from 0.03875\n",
            "Epoch 18/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0425 - val_loss: 0.0245 - val_tf_correlation: 0.0342\n",
            "\n",
            "Epoch 00018: val_tf_correlation did not improve from 0.03875\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 19/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0499 - val_loss: 0.0243 - val_tf_correlation: 0.0403\n",
            "\n",
            "Epoch 00019: val_tf_correlation improved from 0.03875 to 0.04031, saving model to model_0.h5\n",
            "Epoch 20/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0543 - val_loss: 0.0243 - val_tf_correlation: 0.0421\n",
            "\n",
            "Epoch 00020: val_tf_correlation improved from 0.04031 to 0.04213, saving model to model_0.h5\n",
            "Epoch 21/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0535 - val_loss: 0.0243 - val_tf_correlation: 0.0424\n",
            "\n",
            "Epoch 00021: val_tf_correlation improved from 0.04213 to 0.04238, saving model to model_0.h5\n",
            "Epoch 22/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0563 - val_loss: 0.0243 - val_tf_correlation: 0.0414\n",
            "\n",
            "Epoch 00022: val_tf_correlation did not improve from 0.04238\n",
            "Epoch 23/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0570 - val_loss: 0.0243 - val_tf_correlation: 0.0428\n",
            "\n",
            "Epoch 00023: val_tf_correlation improved from 0.04238 to 0.04283, saving model to model_0.h5\n",
            "Epoch 24/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0565 - val_loss: 0.0243 - val_tf_correlation: 0.0409\n",
            "\n",
            "Epoch 00024: val_tf_correlation did not improve from 0.04283\n",
            "Epoch 25/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0566 - val_loss: 0.0243 - val_tf_correlation: 0.0428\n",
            "\n",
            "Epoch 00025: val_tf_correlation did not improve from 0.04283\n",
            "Epoch 26/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0585 - val_loss: 0.0243 - val_tf_correlation: 0.0420\n",
            "\n",
            "Epoch 00026: val_tf_correlation did not improve from 0.04283\n",
            "Epoch 27/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0577 - val_loss: 0.0243 - val_tf_correlation: 0.0420\n",
            "\n",
            "Epoch 00027: val_tf_correlation did not improve from 0.04283\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 28/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0593 - val_loss: 0.0243 - val_tf_correlation: 0.0431\n",
            "\n",
            "Epoch 00028: val_tf_correlation improved from 0.04283 to 0.04313, saving model to model_0.h5\n",
            "Epoch 29/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0604 - val_loss: 0.0243 - val_tf_correlation: 0.0426\n",
            "\n",
            "Epoch 00029: val_tf_correlation did not improve from 0.04313\n",
            "Epoch 30/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0598 - val_loss: 0.0243 - val_tf_correlation: 0.0434\n",
            "\n",
            "Epoch 00030: val_tf_correlation improved from 0.04313 to 0.04345, saving model to model_0.h5\n",
            "Epoch 31/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0585 - val_loss: 0.0243 - val_tf_correlation: 0.0433\n",
            "\n",
            "Epoch 00031: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 32/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0599 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00032: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 33/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0616 - val_loss: 0.0243 - val_tf_correlation: 0.0428\n",
            "\n",
            "Epoch 00033: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 34/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0611 - val_loss: 0.0243 - val_tf_correlation: 0.0433\n",
            "\n",
            "Epoch 00034: val_tf_correlation did not improve from 0.04345\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 35/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0608 - val_loss: 0.0243 - val_tf_correlation: 0.0433\n",
            "\n",
            "Epoch 00035: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 36/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0596 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00036: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 37/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0597 - val_loss: 0.0243 - val_tf_correlation: 0.0433\n",
            "\n",
            "Epoch 00037: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 38/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0599 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00038: val_tf_correlation did not improve from 0.04345\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 39/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0617 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00039: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 40/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0605 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00040: val_tf_correlation did not improve from 0.04345\n",
            "Epoch 00040: early stopping\n",
            "Spearman score for high feature exposure: 0.04406775778065514\n",
            "Spearman score for balanced feature expo: 0.037024978801711524\n",
            "std for high feature exposure: 0.027365452308564835\n",
            "std for balanced feature expo: 0.02230515544504502\n",
            "smart sharpe for high feature exposure: 1.5467314420996912\n",
            "smart sharpe for balanced feature expo: 1.6215301578441002\n",
            "smart sortino for high feature exposure: 3.672564457536672\n",
            "smart sortino for balanced feature expo: 2.1426233649701896\n",
            "Epoch 1/100\n",
            "98/98 - 5s - loss: 0.1312 - tf_correlation: -2.2268e-03 - val_loss: 0.0264 - val_tf_correlation: -6.8676e-04\n",
            "\n",
            "Epoch 00001: val_tf_correlation improved from -inf to -0.00069, saving model to model_1.h5\n",
            "Epoch 2/100\n",
            "98/98 - 3s - loss: 0.0569 - tf_correlation: 0.0039 - val_loss: 0.0247 - val_tf_correlation: 0.0097\n",
            "\n",
            "Epoch 00002: val_tf_correlation improved from -0.00069 to 0.00974, saving model to model_1.h5\n",
            "Epoch 3/100\n",
            "98/98 - 3s - loss: 0.0417 - tf_correlation: 0.0016 - val_loss: 0.0251 - val_tf_correlation: 0.0107\n",
            "\n",
            "Epoch 00003: val_tf_correlation improved from 0.00974 to 0.01065, saving model to model_1.h5\n",
            "Epoch 4/100\n",
            "98/98 - 3s - loss: 0.0328 - tf_correlation: 0.0049 - val_loss: 0.0246 - val_tf_correlation: 0.0074\n",
            "\n",
            "Epoch 00004: val_tf_correlation did not improve from 0.01065\n",
            "Epoch 5/100\n",
            "98/98 - 3s - loss: 0.0287 - tf_correlation: 0.0046 - val_loss: 0.0244 - val_tf_correlation: 0.0134\n",
            "\n",
            "Epoch 00005: val_tf_correlation improved from 0.01065 to 0.01336, saving model to model_1.h5\n",
            "Epoch 6/100\n",
            "98/98 - 3s - loss: 0.0269 - tf_correlation: 0.0015 - val_loss: 0.0243 - val_tf_correlation: 0.0233\n",
            "\n",
            "Epoch 00006: val_tf_correlation improved from 0.01336 to 0.02329, saving model to model_1.h5\n",
            "Epoch 7/100\n",
            "98/98 - 3s - loss: 0.0258 - tf_correlation: 0.0057 - val_loss: 0.0243 - val_tf_correlation: 0.0239\n",
            "\n",
            "Epoch 00007: val_tf_correlation improved from 0.02329 to 0.02388, saving model to model_1.h5\n",
            "Epoch 8/100\n",
            "98/98 - 3s - loss: 0.0253 - tf_correlation: 0.0092 - val_loss: 0.0244 - val_tf_correlation: 0.0223\n",
            "\n",
            "Epoch 00008: val_tf_correlation did not improve from 0.02388\n",
            "Epoch 9/100\n",
            "98/98 - 3s - loss: 0.0251 - tf_correlation: 0.0111 - val_loss: 0.0243 - val_tf_correlation: 0.0286\n",
            "\n",
            "Epoch 00009: val_tf_correlation improved from 0.02388 to 0.02859, saving model to model_1.h5\n",
            "Epoch 10/100\n",
            "98/98 - 3s - loss: 0.0248 - tf_correlation: 0.0117 - val_loss: 0.0244 - val_tf_correlation: 0.0291\n",
            "\n",
            "Epoch 00010: val_tf_correlation improved from 0.02859 to 0.02907, saving model to model_1.h5\n",
            "Epoch 11/100\n",
            "98/98 - 3s - loss: 0.0247 - tf_correlation: 0.0147 - val_loss: 0.0246 - val_tf_correlation: 0.0187\n",
            "\n",
            "Epoch 00011: val_tf_correlation did not improve from 0.02907\n",
            "Epoch 12/100\n",
            "98/98 - 3s - loss: 0.0246 - tf_correlation: 0.0174 - val_loss: 0.0244 - val_tf_correlation: 0.0269\n",
            "\n",
            "Epoch 00012: val_tf_correlation did not improve from 0.02907\n",
            "Epoch 13/100\n",
            "98/98 - 3s - loss: 0.0246 - tf_correlation: 0.0194 - val_loss: 0.0244 - val_tf_correlation: 0.0342\n",
            "\n",
            "Epoch 00013: val_tf_correlation improved from 0.02907 to 0.03415, saving model to model_1.h5\n",
            "Epoch 14/100\n",
            "98/98 - 3s - loss: 0.0245 - tf_correlation: 0.0281 - val_loss: 0.0244 - val_tf_correlation: 0.0318\n",
            "\n",
            "Epoch 00014: val_tf_correlation did not improve from 0.03415\n",
            "Epoch 15/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0350 - val_loss: 0.0244 - val_tf_correlation: 0.0274\n",
            "\n",
            "Epoch 00015: val_tf_correlation did not improve from 0.03415\n",
            "Epoch 16/100\n",
            "98/98 - 3s - loss: 0.0245 - tf_correlation: 0.0317 - val_loss: 0.0245 - val_tf_correlation: 0.0327\n",
            "\n",
            "Epoch 00016: val_tf_correlation did not improve from 0.03415\n",
            "Epoch 17/100\n",
            "98/98 - 3s - loss: 0.0244 - tf_correlation: 0.0376 - val_loss: 0.0245 - val_tf_correlation: 0.0235\n",
            "\n",
            "Epoch 00017: val_tf_correlation did not improve from 0.03415\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 18/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0449 - val_loss: 0.0243 - val_tf_correlation: 0.0356\n",
            "\n",
            "Epoch 00018: val_tf_correlation improved from 0.03415 to 0.03564, saving model to model_1.h5\n",
            "Epoch 19/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0452 - val_loss: 0.0243 - val_tf_correlation: 0.0339\n",
            "\n",
            "Epoch 00019: val_tf_correlation did not improve from 0.03564\n",
            "Epoch 20/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0488 - val_loss: 0.0243 - val_tf_correlation: 0.0363\n",
            "\n",
            "Epoch 00020: val_tf_correlation improved from 0.03564 to 0.03634, saving model to model_1.h5\n",
            "Epoch 21/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0486 - val_loss: 0.0243 - val_tf_correlation: 0.0366\n",
            "\n",
            "Epoch 00021: val_tf_correlation improved from 0.03634 to 0.03656, saving model to model_1.h5\n",
            "Epoch 22/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0519 - val_loss: 0.0243 - val_tf_correlation: 0.0339\n",
            "\n",
            "Epoch 00022: val_tf_correlation did not improve from 0.03656\n",
            "Epoch 23/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0515 - val_loss: 0.0243 - val_tf_correlation: 0.0370\n",
            "\n",
            "Epoch 00023: val_tf_correlation improved from 0.03656 to 0.03698, saving model to model_1.h5\n",
            "Epoch 24/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0496 - val_loss: 0.0243 - val_tf_correlation: 0.0367\n",
            "\n",
            "Epoch 00024: val_tf_correlation did not improve from 0.03698\n",
            "Epoch 25/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0461 - val_loss: 0.0243 - val_tf_correlation: 0.0377\n",
            "\n",
            "Epoch 00025: val_tf_correlation improved from 0.03698 to 0.03771, saving model to model_1.h5\n",
            "Epoch 26/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0507 - val_loss: 0.0243 - val_tf_correlation: 0.0365\n",
            "\n",
            "Epoch 00026: val_tf_correlation did not improve from 0.03771\n",
            "Epoch 27/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0519 - val_loss: 0.0243 - val_tf_correlation: 0.0324\n",
            "\n",
            "Epoch 00027: val_tf_correlation did not improve from 0.03771\n",
            "Epoch 28/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0521 - val_loss: 0.0243 - val_tf_correlation: 0.0376\n",
            "\n",
            "Epoch 00028: val_tf_correlation did not improve from 0.03771\n",
            "Epoch 29/100\n",
            "98/98 - 3s - loss: 0.0243 - tf_correlation: 0.0536 - val_loss: 0.0243 - val_tf_correlation: 0.0363\n",
            "\n",
            "Epoch 00029: val_tf_correlation did not improve from 0.03771\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 30/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0547 - val_loss: 0.0243 - val_tf_correlation: 0.0372\n",
            "\n",
            "Epoch 00030: val_tf_correlation did not improve from 0.03771\n",
            "Epoch 31/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0576 - val_loss: 0.0243 - val_tf_correlation: 0.0382\n",
            "\n",
            "Epoch 00031: val_tf_correlation improved from 0.03771 to 0.03819, saving model to model_1.h5\n",
            "Epoch 32/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0547 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00032: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 33/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0547 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00033: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 34/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0560 - val_loss: 0.0243 - val_tf_correlation: 0.0368\n",
            "\n",
            "Epoch 00034: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 35/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0562 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00035: val_tf_correlation did not improve from 0.03819\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 36/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0555 - val_loss: 0.0243 - val_tf_correlation: 0.0379\n",
            "\n",
            "Epoch 00036: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 37/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0591 - val_loss: 0.0243 - val_tf_correlation: 0.0379\n",
            "\n",
            "Epoch 00037: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 38/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0548 - val_loss: 0.0243 - val_tf_correlation: 0.0380\n",
            "\n",
            "Epoch 00038: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 39/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0555 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00039: val_tf_correlation did not improve from 0.03819\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 40/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0588 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00040: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 41/100\n",
            "98/98 - 3s - loss: 0.0242 - tf_correlation: 0.0555 - val_loss: 0.0243 - val_tf_correlation: 0.0378\n",
            "\n",
            "Epoch 00041: val_tf_correlation did not improve from 0.03819\n",
            "Epoch 00041: early stopping\n",
            "Spearman score for high feature exposure: 0.04012223404003052\n",
            "Spearman score for balanced feature expo: 0.03854775943515707\n",
            "std for high feature exposure: 0.040489521203739776\n",
            "std for balanced feature expo: 0.03203373847451461\n",
            "smart sharpe for high feature exposure: 0.9037268418781719\n",
            "smart sharpe for balanced feature expo: 1.0437222045149772\n",
            "smart sortino for high feature exposure: 0.837928906267048\n",
            "smart sortino for balanced feature expo: 0.9672814701285087\n",
            "Epoch 1/100\n",
            "99/99 - 5s - loss: 0.1147 - tf_correlation: -9.0614e-04 - val_loss: 0.0272 - val_tf_correlation: 0.0085\n",
            "\n",
            "Epoch 00001: val_tf_correlation improved from -inf to 0.00845, saving model to model_2.h5\n",
            "Epoch 2/100\n",
            "99/99 - 3s - loss: 0.0384 - tf_correlation: 0.0034 - val_loss: 0.0249 - val_tf_correlation: 0.0059\n",
            "\n",
            "Epoch 00002: val_tf_correlation did not improve from 0.00845\n",
            "Epoch 3/100\n",
            "99/99 - 3s - loss: 0.0288 - tf_correlation: 0.0012 - val_loss: 0.0246 - val_tf_correlation: 0.0063\n",
            "\n",
            "Epoch 00003: val_tf_correlation did not improve from 0.00845\n",
            "Epoch 4/100\n",
            "99/99 - 3s - loss: 0.0262 - tf_correlation: 0.0047 - val_loss: 0.0246 - val_tf_correlation: 0.0190\n",
            "\n",
            "Epoch 00004: val_tf_correlation improved from 0.00845 to 0.01902, saving model to model_2.h5\n",
            "Epoch 5/100\n",
            "99/99 - 3s - loss: 0.0253 - tf_correlation: 0.0070 - val_loss: 0.0251 - val_tf_correlation: 0.0172\n",
            "\n",
            "Epoch 00005: val_tf_correlation did not improve from 0.01902\n",
            "Epoch 6/100\n",
            "99/99 - 3s - loss: 0.0249 - tf_correlation: 0.0092 - val_loss: 0.0254 - val_tf_correlation: 0.0187\n",
            "\n",
            "Epoch 00006: val_tf_correlation did not improve from 0.01902\n",
            "Epoch 7/100\n",
            "99/99 - 3s - loss: 0.0248 - tf_correlation: 0.0098 - val_loss: 0.0244 - val_tf_correlation: 0.0221\n",
            "\n",
            "Epoch 00007: val_tf_correlation improved from 0.01902 to 0.02213, saving model to model_2.h5\n",
            "Epoch 8/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0154 - val_loss: 0.0246 - val_tf_correlation: 0.0180\n",
            "\n",
            "Epoch 00008: val_tf_correlation did not improve from 0.02213\n",
            "Epoch 9/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0154 - val_loss: 0.0243 - val_tf_correlation: 0.0203\n",
            "\n",
            "Epoch 00009: val_tf_correlation did not improve from 0.02213\n",
            "Epoch 10/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0191 - val_loss: 0.0247 - val_tf_correlation: 0.0208\n",
            "\n",
            "Epoch 00010: val_tf_correlation did not improve from 0.02213\n",
            "Epoch 11/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0220 - val_loss: 0.0244 - val_tf_correlation: 0.0286\n",
            "\n",
            "Epoch 00011: val_tf_correlation improved from 0.02213 to 0.02863, saving model to model_2.h5\n",
            "Epoch 12/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0271 - val_loss: 0.0244 - val_tf_correlation: 0.0260\n",
            "\n",
            "Epoch 00012: val_tf_correlation did not improve from 0.02863\n",
            "Epoch 13/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0291 - val_loss: 0.0268 - val_tf_correlation: 0.0054\n",
            "\n",
            "Epoch 00013: val_tf_correlation did not improve from 0.02863\n",
            "Epoch 14/100\n",
            "99/99 - 3s - loss: 0.0246 - tf_correlation: 0.0281 - val_loss: 0.0244 - val_tf_correlation: 0.0345\n",
            "\n",
            "Epoch 00014: val_tf_correlation improved from 0.02863 to 0.03451, saving model to model_2.h5\n",
            "Epoch 15/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0301 - val_loss: 0.0247 - val_tf_correlation: 0.0326\n",
            "\n",
            "Epoch 00015: val_tf_correlation did not improve from 0.03451\n",
            "Epoch 16/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0370 - val_loss: 0.0248 - val_tf_correlation: 0.0264\n",
            "\n",
            "Epoch 00016: val_tf_correlation did not improve from 0.03451\n",
            "Epoch 17/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0330 - val_loss: 0.0246 - val_tf_correlation: 0.0225\n",
            "\n",
            "Epoch 00017: val_tf_correlation did not improve from 0.03451\n",
            "Epoch 18/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0406 - val_loss: 0.0246 - val_tf_correlation: 0.0323\n",
            "\n",
            "Epoch 00018: val_tf_correlation did not improve from 0.03451\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 19/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0476 - val_loss: 0.0242 - val_tf_correlation: 0.0422\n",
            "\n",
            "Epoch 00019: val_tf_correlation improved from 0.03451 to 0.04224, saving model to model_2.h5\n",
            "Epoch 20/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0508 - val_loss: 0.0242 - val_tf_correlation: 0.0430\n",
            "\n",
            "Epoch 00020: val_tf_correlation improved from 0.04224 to 0.04297, saving model to model_2.h5\n",
            "Epoch 21/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0528 - val_loss: 0.0243 - val_tf_correlation: 0.0416\n",
            "\n",
            "Epoch 00021: val_tf_correlation did not improve from 0.04297\n",
            "Epoch 22/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0540 - val_loss: 0.0243 - val_tf_correlation: 0.0417\n",
            "\n",
            "Epoch 00022: val_tf_correlation did not improve from 0.04297\n",
            "Epoch 23/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0561 - val_loss: 0.0243 - val_tf_correlation: 0.0435\n",
            "\n",
            "Epoch 00023: val_tf_correlation improved from 0.04297 to 0.04353, saving model to model_2.h5\n",
            "Epoch 24/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0524 - val_loss: 0.0243 - val_tf_correlation: 0.0432\n",
            "\n",
            "Epoch 00024: val_tf_correlation did not improve from 0.04353\n",
            "Epoch 25/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0519 - val_loss: 0.0242 - val_tf_correlation: 0.0448\n",
            "\n",
            "Epoch 00025: val_tf_correlation improved from 0.04353 to 0.04482, saving model to model_2.h5\n",
            "Epoch 26/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0558 - val_loss: 0.0243 - val_tf_correlation: 0.0448\n",
            "\n",
            "Epoch 00026: val_tf_correlation did not improve from 0.04482\n",
            "Epoch 27/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0555 - val_loss: 0.0245 - val_tf_correlation: 0.0408\n",
            "\n",
            "Epoch 00027: val_tf_correlation did not improve from 0.04482\n",
            "Epoch 28/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0545 - val_loss: 0.0242 - val_tf_correlation: 0.0449\n",
            "\n",
            "Epoch 00028: val_tf_correlation improved from 0.04482 to 0.04491, saving model to model_2.h5\n",
            "Epoch 29/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0562 - val_loss: 0.0243 - val_tf_correlation: 0.0449\n",
            "\n",
            "Epoch 00029: val_tf_correlation did not improve from 0.04491\n",
            "Epoch 30/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0549 - val_loss: 0.0243 - val_tf_correlation: 0.0460\n",
            "\n",
            "Epoch 00030: val_tf_correlation improved from 0.04491 to 0.04604, saving model to model_2.h5\n",
            "Epoch 31/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0588 - val_loss: 0.0243 - val_tf_correlation: 0.0420\n",
            "\n",
            "Epoch 00031: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 32/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0566 - val_loss: 0.0242 - val_tf_correlation: 0.0453\n",
            "\n",
            "Epoch 00032: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 33/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0576 - val_loss: 0.0243 - val_tf_correlation: 0.0450\n",
            "\n",
            "Epoch 00033: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 34/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0581 - val_loss: 0.0242 - val_tf_correlation: 0.0445\n",
            "\n",
            "Epoch 00034: val_tf_correlation did not improve from 0.04604\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 35/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0603 - val_loss: 0.0242 - val_tf_correlation: 0.0450\n",
            "\n",
            "Epoch 00035: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 36/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0632 - val_loss: 0.0242 - val_tf_correlation: 0.0449\n",
            "\n",
            "Epoch 00036: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 37/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0593 - val_loss: 0.0242 - val_tf_correlation: 0.0449\n",
            "\n",
            "Epoch 00037: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 38/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0606 - val_loss: 0.0243 - val_tf_correlation: 0.0451\n",
            "\n",
            "Epoch 00038: val_tf_correlation did not improve from 0.04604\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 39/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0615 - val_loss: 0.0242 - val_tf_correlation: 0.0450\n",
            "\n",
            "Epoch 00039: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 40/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0604 - val_loss: 0.0242 - val_tf_correlation: 0.0450\n",
            "\n",
            "Epoch 00040: val_tf_correlation did not improve from 0.04604\n",
            "Epoch 00040: early stopping\n",
            "Spearman score for high feature exposure: 0.04625575816760666\n",
            "Spearman score for balanced feature expo: 0.03740425426797513\n",
            "std for high feature exposure: 0.02438213889525093\n",
            "std for balanced feature expo: 0.023753668837702756\n",
            "smart sharpe for high feature exposure: 1.6625582705854305\n",
            "smart sharpe for balanced feature expo: 1.3299515952777876\n",
            "smart sortino for high feature exposure: 9.836382057650479\n",
            "smart sortino for balanced feature expo: 2.636219013205247\n",
            "Epoch 1/100\n",
            "99/99 - 5s - loss: 0.1488 - tf_correlation: 3.3807e-04 - val_loss: 0.2203 - val_tf_correlation: 0.0076\n",
            "\n",
            "Epoch 00001: val_tf_correlation improved from -inf to 0.00757, saving model to model_3.h5\n",
            "Epoch 2/100\n",
            "99/99 - 3s - loss: 0.0676 - tf_correlation: 0.0033 - val_loss: 0.0259 - val_tf_correlation: -3.9329e-03\n",
            "\n",
            "Epoch 00002: val_tf_correlation did not improve from 0.00757\n",
            "Epoch 3/100\n",
            "99/99 - 3s - loss: 0.0539 - tf_correlation: -1.7704e-03 - val_loss: 0.0273 - val_tf_correlation: 0.0062\n",
            "\n",
            "Epoch 00003: val_tf_correlation did not improve from 0.00757\n",
            "Epoch 4/100\n",
            "99/99 - 3s - loss: 0.0434 - tf_correlation: 0.0029 - val_loss: 0.0257 - val_tf_correlation: 0.0027\n",
            "\n",
            "Epoch 00004: val_tf_correlation did not improve from 0.00757\n",
            "Epoch 5/100\n",
            "99/99 - 3s - loss: 0.0359 - tf_correlation: 0.0042 - val_loss: 0.0260 - val_tf_correlation: 0.0134\n",
            "\n",
            "Epoch 00005: val_tf_correlation improved from 0.00757 to 0.01335, saving model to model_3.h5\n",
            "Epoch 6/100\n",
            "99/99 - 3s - loss: 0.0313 - tf_correlation: 0.0018 - val_loss: 0.0271 - val_tf_correlation: 0.0181\n",
            "\n",
            "Epoch 00006: val_tf_correlation improved from 0.01335 to 0.01810, saving model to model_3.h5\n",
            "Epoch 7/100\n",
            "99/99 - 3s - loss: 0.0290 - tf_correlation: 0.0066 - val_loss: 0.0247 - val_tf_correlation: 0.0143\n",
            "\n",
            "Epoch 00007: val_tf_correlation did not improve from 0.01810\n",
            "Epoch 8/100\n",
            "99/99 - 3s - loss: 0.0272 - tf_correlation: 0.0044 - val_loss: 0.0244 - val_tf_correlation: 0.0222\n",
            "\n",
            "Epoch 00008: val_tf_correlation improved from 0.01810 to 0.02219, saving model to model_3.h5\n",
            "Epoch 9/100\n",
            "99/99 - 3s - loss: 0.0263 - tf_correlation: 0.0025 - val_loss: 0.0245 - val_tf_correlation: 0.0242\n",
            "\n",
            "Epoch 00009: val_tf_correlation improved from 0.02219 to 0.02424, saving model to model_3.h5\n",
            "Epoch 10/100\n",
            "99/99 - 3s - loss: 0.0258 - tf_correlation: 0.0082 - val_loss: 0.0259 - val_tf_correlation: 0.0266\n",
            "\n",
            "Epoch 00010: val_tf_correlation improved from 0.02424 to 0.02665, saving model to model_3.h5\n",
            "Epoch 11/100\n",
            "99/99 - 3s - loss: 0.0258 - tf_correlation: 0.0076 - val_loss: 0.0247 - val_tf_correlation: 0.0133\n",
            "\n",
            "Epoch 00011: val_tf_correlation did not improve from 0.02665\n",
            "Epoch 12/100\n",
            "99/99 - 3s - loss: 0.0255 - tf_correlation: 0.0128 - val_loss: 0.0246 - val_tf_correlation: 0.0160\n",
            "\n",
            "Epoch 00012: val_tf_correlation did not improve from 0.02665\n",
            "Epoch 13/100\n",
            "99/99 - 3s - loss: 0.0252 - tf_correlation: 0.0097 - val_loss: 0.0253 - val_tf_correlation: 0.0226\n",
            "\n",
            "Epoch 00013: val_tf_correlation did not improve from 0.02665\n",
            "Epoch 14/100\n",
            "99/99 - 3s - loss: 0.0251 - tf_correlation: 0.0178 - val_loss: 0.0254 - val_tf_correlation: 0.0202\n",
            "\n",
            "Epoch 00014: val_tf_correlation did not improve from 0.02665\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 15/100\n",
            "99/99 - 3s - loss: 0.0249 - tf_correlation: 0.0206 - val_loss: 0.0243 - val_tf_correlation: 0.0332\n",
            "\n",
            "Epoch 00015: val_tf_correlation improved from 0.02665 to 0.03320, saving model to model_3.h5\n",
            "Epoch 16/100\n",
            "99/99 - 3s - loss: 0.0248 - tf_correlation: 0.0215 - val_loss: 0.0243 - val_tf_correlation: 0.0305\n",
            "\n",
            "Epoch 00016: val_tf_correlation did not improve from 0.03320\n",
            "Epoch 17/100\n",
            "99/99 - 3s - loss: 0.0248 - tf_correlation: 0.0143 - val_loss: 0.0243 - val_tf_correlation: 0.0332\n",
            "\n",
            "Epoch 00017: val_tf_correlation improved from 0.03320 to 0.03321, saving model to model_3.h5\n",
            "Epoch 18/100\n",
            "99/99 - 3s - loss: 0.0248 - tf_correlation: 0.0155 - val_loss: 0.0243 - val_tf_correlation: 0.0313\n",
            "\n",
            "Epoch 00018: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 19/100\n",
            "99/99 - 3s - loss: 0.0248 - tf_correlation: 0.0187 - val_loss: 0.0243 - val_tf_correlation: 0.0305\n",
            "\n",
            "Epoch 00019: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 20/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0229 - val_loss: 0.0243 - val_tf_correlation: 0.0300\n",
            "\n",
            "Epoch 00020: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 21/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0198 - val_loss: 0.0243 - val_tf_correlation: 0.0276\n",
            "\n",
            "Epoch 00021: val_tf_correlation did not improve from 0.03321\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 22/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0206 - val_loss: 0.0243 - val_tf_correlation: 0.0300\n",
            "\n",
            "Epoch 00022: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 23/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0229 - val_loss: 0.0243 - val_tf_correlation: 0.0309\n",
            "\n",
            "Epoch 00023: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 24/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0195 - val_loss: 0.0243 - val_tf_correlation: 0.0299\n",
            "\n",
            "Epoch 00024: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 25/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0194 - val_loss: 0.0243 - val_tf_correlation: 0.0305\n",
            "\n",
            "Epoch 00025: val_tf_correlation did not improve from 0.03321\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 26/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0229 - val_loss: 0.0243 - val_tf_correlation: 0.0315\n",
            "\n",
            "Epoch 00026: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 27/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0219 - val_loss: 0.0243 - val_tf_correlation: 0.0313\n",
            "\n",
            "Epoch 00027: val_tf_correlation did not improve from 0.03321\n",
            "Epoch 00027: early stopping\n",
            "Spearman score for high feature exposure: 0.03457603013707879\n",
            "Spearman score for balanced feature expo: 0.030129737541389524\n",
            "std for high feature exposure: 0.02610203367224574\n",
            "std for balanced feature expo: 0.020228586727092403\n",
            "smart sharpe for high feature exposure: 0.9487937677778867\n",
            "smart sharpe for balanced feature expo: 1.1388760291448772\n",
            "smart sortino for high feature exposure: 0.8806990314613149\n",
            "smart sortino for balanced feature expo: 0.9175196652762578\n",
            "Epoch 1/100\n",
            "99/99 - 5s - loss: 0.1032 - tf_correlation: -1.9665e-03 - val_loss: 0.0300 - val_tf_correlation: 0.0031\n",
            "\n",
            "Epoch 00001: val_tf_correlation improved from -inf to 0.00307, saving model to model_4.h5\n",
            "Epoch 2/100\n",
            "99/99 - 3s - loss: 0.0336 - tf_correlation: 0.0056 - val_loss: 0.0259 - val_tf_correlation: -3.9543e-03\n",
            "\n",
            "Epoch 00002: val_tf_correlation did not improve from 0.00307\n",
            "Epoch 3/100\n",
            "99/99 - 3s - loss: 0.0265 - tf_correlation: 0.0052 - val_loss: 0.0255 - val_tf_correlation: 0.0099\n",
            "\n",
            "Epoch 00003: val_tf_correlation improved from 0.00307 to 0.00987, saving model to model_4.h5\n",
            "Epoch 4/100\n",
            "99/99 - 3s - loss: 0.0252 - tf_correlation: 0.0073 - val_loss: 0.0261 - val_tf_correlation: 0.0045\n",
            "\n",
            "Epoch 00004: val_tf_correlation did not improve from 0.00987\n",
            "Epoch 5/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0117 - val_loss: 0.0255 - val_tf_correlation: 0.0114\n",
            "\n",
            "Epoch 00005: val_tf_correlation improved from 0.00987 to 0.01144, saving model to model_4.h5\n",
            "Epoch 6/100\n",
            "99/99 - 3s - loss: 0.0246 - tf_correlation: 0.0129 - val_loss: 0.0278 - val_tf_correlation: 0.0070\n",
            "\n",
            "Epoch 00006: val_tf_correlation did not improve from 0.01144\n",
            "Epoch 7/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0148 - val_loss: 0.0246 - val_tf_correlation: 0.0241\n",
            "\n",
            "Epoch 00007: val_tf_correlation improved from 0.01144 to 0.02409, saving model to model_4.h5\n",
            "Epoch 8/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0177 - val_loss: 0.0268 - val_tf_correlation: 0.0155\n",
            "\n",
            "Epoch 00008: val_tf_correlation did not improve from 0.02409\n",
            "Epoch 9/100\n",
            "99/99 - 3s - loss: 0.0247 - tf_correlation: 0.0213 - val_loss: 0.0245 - val_tf_correlation: 0.0089\n",
            "\n",
            "Epoch 00009: val_tf_correlation did not improve from 0.02409\n",
            "Epoch 10/100\n",
            "99/99 - 3s - loss: 0.0245 - tf_correlation: 0.0268 - val_loss: 0.0245 - val_tf_correlation: 0.0191\n",
            "\n",
            "Epoch 00010: val_tf_correlation did not improve from 0.02409\n",
            "Epoch 11/100\n",
            "99/99 - 3s - loss: 0.0244 - tf_correlation: 0.0297 - val_loss: 0.0250 - val_tf_correlation: 0.0203\n",
            "\n",
            "Epoch 00011: val_tf_correlation did not improve from 0.02409\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 12/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0390 - val_loss: 0.0243 - val_tf_correlation: 0.0388\n",
            "\n",
            "Epoch 00012: val_tf_correlation improved from 0.02409 to 0.03884, saving model to model_4.h5\n",
            "Epoch 13/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0425 - val_loss: 0.0243 - val_tf_correlation: 0.0384\n",
            "\n",
            "Epoch 00013: val_tf_correlation did not improve from 0.03884\n",
            "Epoch 14/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0383 - val_loss: 0.0243 - val_tf_correlation: 0.0421\n",
            "\n",
            "Epoch 00014: val_tf_correlation improved from 0.03884 to 0.04214, saving model to model_4.h5\n",
            "Epoch 15/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0390 - val_loss: 0.0243 - val_tf_correlation: 0.0402\n",
            "\n",
            "Epoch 00015: val_tf_correlation did not improve from 0.04214\n",
            "Epoch 16/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0452 - val_loss: 0.0244 - val_tf_correlation: 0.0387\n",
            "\n",
            "Epoch 00016: val_tf_correlation did not improve from 0.04214\n",
            "Epoch 17/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0412 - val_loss: 0.0243 - val_tf_correlation: 0.0403\n",
            "\n",
            "Epoch 00017: val_tf_correlation did not improve from 0.04214\n",
            "Epoch 18/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0445 - val_loss: 0.0243 - val_tf_correlation: 0.0408\n",
            "\n",
            "Epoch 00018: val_tf_correlation did not improve from 0.04214\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 19/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0482 - val_loss: 0.0243 - val_tf_correlation: 0.0419\n",
            "\n",
            "Epoch 00019: val_tf_correlation did not improve from 0.04214\n",
            "Epoch 20/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0502 - val_loss: 0.0243 - val_tf_correlation: 0.0425\n",
            "\n",
            "Epoch 00020: val_tf_correlation improved from 0.04214 to 0.04246, saving model to model_4.h5\n",
            "Epoch 21/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0464 - val_loss: 0.0243 - val_tf_correlation: 0.0426\n",
            "\n",
            "Epoch 00021: val_tf_correlation improved from 0.04246 to 0.04257, saving model to model_4.h5\n",
            "Epoch 22/100\n",
            "99/99 - 3s - loss: 0.0243 - tf_correlation: 0.0463 - val_loss: 0.0243 - val_tf_correlation: 0.0427\n",
            "\n",
            "Epoch 00022: val_tf_correlation improved from 0.04257 to 0.04273, saving model to model_4.h5\n",
            "Epoch 23/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0485 - val_loss: 0.0243 - val_tf_correlation: 0.0427\n",
            "\n",
            "Epoch 00023: val_tf_correlation did not improve from 0.04273\n",
            "Epoch 24/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0493 - val_loss: 0.0243 - val_tf_correlation: 0.0418\n",
            "\n",
            "Epoch 00024: val_tf_correlation did not improve from 0.04273\n",
            "Epoch 25/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0457 - val_loss: 0.0243 - val_tf_correlation: 0.0428\n",
            "\n",
            "Epoch 00025: val_tf_correlation improved from 0.04273 to 0.04281, saving model to model_4.h5\n",
            "Epoch 26/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0474 - val_loss: 0.0243 - val_tf_correlation: 0.0427\n",
            "\n",
            "Epoch 00026: val_tf_correlation did not improve from 0.04281\n",
            "Epoch 27/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0502 - val_loss: 0.0243 - val_tf_correlation: 0.0426\n",
            "\n",
            "Epoch 00027: val_tf_correlation did not improve from 0.04281\n",
            "Epoch 28/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0484 - val_loss: 0.0243 - val_tf_correlation: 0.0427\n",
            "\n",
            "Epoch 00028: val_tf_correlation did not improve from 0.04281\n",
            "Epoch 29/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0519 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00029: val_tf_correlation improved from 0.04281 to 0.04292, saving model to model_4.h5\n",
            "Epoch 30/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0495 - val_loss: 0.0243 - val_tf_correlation: 0.0423\n",
            "\n",
            "Epoch 00030: val_tf_correlation did not improve from 0.04292\n",
            "Epoch 31/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0508 - val_loss: 0.0243 - val_tf_correlation: 0.0424\n",
            "\n",
            "Epoch 00031: val_tf_correlation did not improve from 0.04292\n",
            "Epoch 32/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0496 - val_loss: 0.0243 - val_tf_correlation: 0.0428\n",
            "\n",
            "Epoch 00032: val_tf_correlation did not improve from 0.04292\n",
            "Epoch 33/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0507 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00033: val_tf_correlation did not improve from 0.04292\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 34/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0541 - val_loss: 0.0243 - val_tf_correlation: 0.0431\n",
            "\n",
            "Epoch 00034: val_tf_correlation improved from 0.04292 to 0.04307, saving model to model_4.h5\n",
            "Epoch 35/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0529 - val_loss: 0.0243 - val_tf_correlation: 0.0430\n",
            "\n",
            "Epoch 00035: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 36/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0510 - val_loss: 0.0243 - val_tf_correlation: 0.0431\n",
            "\n",
            "Epoch 00036: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 37/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0540 - val_loss: 0.0243 - val_tf_correlation: 0.0430\n",
            "\n",
            "Epoch 00037: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 38/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0508 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00038: val_tf_correlation did not improve from 0.04307\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 39/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0536 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00039: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 40/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0464 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00040: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 41/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0501 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00041: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 42/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0555 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00042: val_tf_correlation did not improve from 0.04307\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 43/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0495 - val_loss: 0.0243 - val_tf_correlation: 0.0429\n",
            "\n",
            "Epoch 00043: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 44/100\n",
            "99/99 - 3s - loss: 0.0242 - tf_correlation: 0.0519 - val_loss: 0.0243 - val_tf_correlation: 0.0430\n",
            "\n",
            "Epoch 00044: val_tf_correlation did not improve from 0.04307\n",
            "Epoch 00044: early stopping\n",
            "Spearman score for high feature exposure: 0.044289446646220997\n",
            "Spearman score for balanced feature expo: 0.04206128668854644\n",
            "std for high feature exposure: 0.03235402357712483\n",
            "std for balanced feature expo: 0.02791285221659197\n",
            "smart sharpe for high feature exposure: 1.2674518253966902\n",
            "smart sharpe for balanced feature expo: 1.419100022833653\n",
            "smart sortino for high feature exposure: 1.584624421109761\n",
            "smart sortino for balanced feature expo: 1.708627362587553\n",
            "Score for fold 1 is : 0.04406775778065514\n",
            "Score for fold 2 is : 0.04012223404003052\n",
            "Score for fold 3 is : 0.04625575816760666\n",
            "Score for fold 4 is : 0.03457603013707879\n",
            "Score for fold 5 is : 0.044289446646220997\n",
            "Final Score is : 0.04186224535431842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uU5coHuF3p08"
      },
      "source": [
        "# function ClickConnect(){\n",
        "\n",
        "# console.log(\"Working\"); \n",
        "# document.querySelector(\"#comments > span\").click() \n",
        "# }\n",
        "# setInterval(ClickConnect,5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxx5pLIGIUvN"
      },
      "source": [
        "## 4. Generate your first predictions\n",
        "Now that we have a trained model, we can use it to make predictions on the tournament data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5H_FKck79na"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjVKmJK6vXN0",
        "outputId": "8154fea6-abdf-4c12-96f9-85b0a1fb6cd8"
      },
      "source": [
        "try:\n",
        "  del training_data,X,y\n",
        "  del tournament_data\n",
        "except NameError:\n",
        "  pass\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "2UW_K3zLtw9V",
        "outputId": "1b06c951-f0e8-4a93-f051-25e52fc49328"
      },
      "source": [
        "tournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\",nrows=1)\n",
        "dtype_dict = {x:np.float32 for x in tournament_data.columns if ('feature' in x or 'target' in x)}\n",
        "#Not using float16 because np.linalg in _neutralize function does not support it.\n",
        "\n",
        "tournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\n",
        "#tournament data 'validation':121 to 135 eras, 'test':others,'live':eraX\n",
        "\n",
        "#Centering all features and target data around 0 (for good NN convergence)\n",
        "#tournament_data['target'] -= 0.5\n",
        "\n",
        "#Using .loc and mask for below operation uses a lot of  memory \n",
        "for col in (x for x in tournament_data.columns if 'feature' in x):\n",
        "  tournament_data[col] -= 0.5\n",
        "\n",
        "tournament_data = tournament_data.astype(dtype_dict)\n",
        "\n",
        "tournament_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     era  ... feature_wisdom46  target\n",
              "0  n0003aa52cab36c2  era121  ...            -0.50    0.25\n",
              "1  n000920ed083903f  era121  ...             0.00    0.50\n",
              "2  n0038e640522c4a6  era121  ...            -0.50    1.00\n",
              "3  n004ac94a87dc54b  era121  ...            -0.25    0.50\n",
              "4  n0052fe97ea0c05f  era121  ...             0.50    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIcmOiJYSmJI"
      },
      "source": [
        "# select the feature columns from the tournament data\n",
        "features = [x for x in tournament_data.columns if 'feature' in x]\n",
        "live_features = tournament_data[features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "t4CoaCgbKcSN",
        "outputId": "65966b3e-01b4-48c9-f442-e64b75c67d61"
      },
      "source": [
        "# predict the target on the live features\n",
        "num_models = 5\n",
        "predictions = np.zeros((len(tournament_data),1))\n",
        "X_shape = 310 #X.shape[-1]\n",
        "y_shape = 1 #y.shape[-1]\n",
        "\n",
        "\n",
        "_, encoder = create_autoencoder(X_shape,y_shape,noise=0.1)\n",
        "encoder.load_weights(f'encoder.hdf5')\n",
        "encoder.trainable = False\n",
        "\n",
        "for file_name in list(filter(lambda x: 'model' in x,os.listdir('.')))[:num_models]:        \n",
        "    \n",
        "    clf = create_model(X_shape,y_shape,params,encoder)\n",
        "    clf.compile(optimizer=Adam(learning_rate = params['lr']),loss=LogCosh(),metrics=[tf_correlation],\n",
        "                  #run_eagerly=True\n",
        "                  )\n",
        "    clf.load_weights(os.path.join('./',file_name))\n",
        "    \n",
        "    predictions += clf.predict(live_features)\n",
        "    del clf\n",
        "    gc.collect()\n",
        "    print(f\"{file_name} :: DONE\")\n",
        "\n",
        "predictions /= num_models\n",
        "\n",
        "predictions += 0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2e9c4604da10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     clf.compile(optimizer=Adam(learning_rate = params['lr']),loss=LogCosh(),metrics=[tf_correlation],\n\u001b[0m\u001b[1;32m     16\u001b[0m                   \u001b[0;31m#run_eagerly=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_correlation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMlZ_1QiwQeo"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ukqHCoy0Jo"
      },
      "source": [
        "df2 = tournament_data\n",
        "\n",
        "df2[\"preds\"] = predictions\n",
        "df2[\"preds_neutralized\"] = df2.groupby(\"era\").apply(\n",
        "lambda x: normalize_and_neutralize(x, [\"preds\"], features, 0.5) # neutralize by 50% within each era\n",
        ")\n",
        "scaler = MinMaxScaler()\n",
        "df2[\"preds_neutralized\"] = scaler.fit_transform(df2[[\"preds_neutralized\"]]) # transform back to 0-1\n",
        "\n",
        "#df2 = tournament_data[tournament_data['data_type'] == 'validation']\n",
        "\n",
        "spearman_unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds\"]))\n",
        "spearman_balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: correlation(d[\"target\"], d[\"preds_neutralized\"]))\n",
        "\n",
        "\n",
        "print(f\"Spearman score for high feature exposure: {spearman_unbalanced_scores_per_era.mean()}\")\n",
        "print(f\"Spearman score for balanced feature expo: {spearman_balanced_scores_per_era.mean()}\")\n",
        "\n",
        "print(f\"std for high feature exposure: {spearman_unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "print(f\"std for balanced feature expo: {spearman_balanced_scores_per_era.std(ddof=0)}\")\n",
        "\n",
        "try:\n",
        "  print(f\"smart sharpe for high feature exposure: {smart_sharpe(spearman_unbalanced_scores_per_era)}\")\n",
        "  print(f\"smart sharpe for balanced feature expo: {smart_sharpe(spearman_balanced_scores_per_era)}\")\n",
        "except ZeroDivisionError:\n",
        "  print(\"Division by zero!!!\")\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L4BFeAzTTDe"
      },
      "source": [
        "# predictions must have an `id` column and a `prediction_kazutsugi` column\n",
        "predictions_df = tournament_data[\"id\"].to_frame()\n",
        "predictions_df[\"prediction_kazutsugi\"] = df2[\"preds_neutralized\"]\n",
        "predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVijSzwOsLdX"
      },
      "source": [
        "predictions_df[\"prediction_kazutsugi\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0sgtPiLDys"
      },
      "source": [
        "## 5. Make your first submission\n",
        "To enter the tournament, we must submit the predictions back to Numerai. We will use the `numerapi` library to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEfqpxcEWDdK"
      },
      "source": [
        "# Get your API keys and model_id from https://numer.ai/submit\n",
        "public_id = \"BX5I7ZCKH3HK2U337GQSTKIPQWEBPLTQ\"\n",
        "secret_key = \"DJK3BNVUPQNY2IAWCEHEPLEGFEXD2K6U6SOHA67F7U4U2MA7KJXED3XRAAEU4SMP\"\n",
        "model_id = \"37686618-db61-44d0-ade0-30d176beedd0\"\n",
        "napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAIJHaoW3VU"
      },
      "source": [
        "#Upload your predictions\n",
        "predictions_df.to_csv(\"predictions.csv\", index=False)\n",
        "submission_id = napi.upload_predictions(\"predictions.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRSYQ3JonyXt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}